@inproceedings{usaarsemeval2016,
title={{USAAR at SemEval-2016 Task 13: Hyponym Endocentricity}},
author={Tan, Liling and Bond, Francis and van Genabith, Josef},
booktitle={Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016)},
year = {2016},
organization={Association for Computational Linguistics}
}


@inproceedings{nuigsemeval2016,
title={{NUIG-UNLP at SemEval-2016 Task 13: A Simple Word Embedding-based Approach for Taxonomy Extraction}},
author={Pocostales, Joel},
booktitle={Proceedings of the 10th International Workshop on Semantic Evaluation},
year = {2016},
organization={Association for Computational Linguistics}
}


@inproceedings{panchenko2016taxi,
title={{TAXI at SemEval-2016 Task 13: a Taxonomy Induction Method based on Lexico-Syntactic Patterns, Substrings and Focused Crawling}},
author={Panchenko, Alexander Faralli, Stefano Ruppert, Eugen Remus, Steffen Naets, Hubert Fairon, Cedrick Ponzetto, Simone Paolo and Biemann, Chris},
booktitle={Proceedings of the 10th International Workshop on Semantic Evaluation},
year={2016},
organization={Association for Computational Linguistics}
}

@inproceedings{qassitsemeval2016,
title={{QASSIT at SemEval-2016 Task 13: On the integration of Semantic Vectors in Pretopological Spaces for Lexical Taxonomy Acquisition}},
author={Cleuziou, Guillaume and Moreno, Jose G.},
booktitle={Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016)},
month = {June},
year = {2016},
organization={Association for Computational Linguistics}
}


@article{grefenstette2015inriasac,
  title={{INRIASAC}: Simple Hypernym Extraction Methods},
  author={Grefenstette, Gregory},
  journal={SemEval-2015},
  pages={911--914},
  year={2015}
}

@inproceedings{kneser1995improved,
  title={Improved backing-off for m-gram language modeling},
  author={Kneser, Reinhard and Ney, Hermann},
  booktitle={Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on},
  volume={1},
  pages={181--184},
  year={1995},
  organization={IEEE}
}

@inproceedings{toutanova2003feature,
  title={Feature-rich part-of-speech tagging with a cyclic dependency network},
  author={Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
  booktitle={Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1},
  pages={173--180},
  year={2003},
  organization={Association for Computational Linguistics}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}



@incollection{vert2004primer,
  title={A primer on kernel methods},
  author={Vert, Jean-Philippe and Tsuda, Koji and Sch{\"o}lkopf, Bernhard},
  booktitle={Kernel Methods in Computational Biology},
  editor={ Bernhard Sch√∂lkopf and Koji Tsuda and Jean-Philippe Vert},
  pages={35--70},
  year={2004},
  publisher={Cambridge, MA: MIT Press}
}

@book{parker2009english,
  title={English gigaword fourth edition},
  author={Robert Parker and David Graff and Junbo Kong and Ke Chen and Kazuaki Maeda},
  year={2009},
  publisher={Linguistic Data Consortium},
  address={Philadelphia, USA}
}

@inproceedings{ferraresi2008introducing,
  title={Introducing and evaluating {ukWaC}, a very large web-derived corpus of {English}},
  author={Ferraresi, Adriano and Zanchetta, Eros and Baroni, Marco and Bernardini, Silvia},
  booktitle={Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google},
  pages={47--54},
  year={2008},
  address={Marakech, Morocco}
}

@string( acl-85 = {Proceedings of the 23th Annual Meeting of
                  the Association for Computational Linguistics,
                  {\em Chicago, Ill., 8--12 July 1985}})

@string( acl-85s = {Proceedings of ACL-85})

@string( acl-95 = {Proceedings of the 33rd Annual Meeting of the
                  Association for Computational Linguistics,
                  {\em Cambridge, Mass., 26--30 June 1995}}) 

@string( acl-95s = {Proceedings of ACL-95})

@string( acl-96 = {Proceedings of the 34th Annual Meeting of the
                  Association for Computational Linguistics, {\em Santa
                  Cruz, Cal., 24--27 June 1996}}) 

@string( acl-96s = {Proceedings of ACL-96})

@string( acl-99 = {Proceedings of the 37th Annual Meeting of the
                  Association for Computational Linguistics, {\em
                  College Park, Md., 20--26 June 1999}}) 

@string( acl-99s = {Proceedings of ACL-99})

@string( acl-02 = {Proceedings of the 40th Annual Meeting of the
                  Association for Computational Linguistics, {\em
                  Philadelphia, Penn., 7--12 July 2002}}) 

@string( acl-02s = {Proceedings of ACL-02})

@string( acl-03 = {Proceedings of the 41st Annual Meeting of the
                  Association for Computational Linguistics (ACL), Sapporo, Japan}) 

@string( acl-03s = {Proceedings of ACL-03})

@string( acl-04 = {Proceedings of the 42nd Annual Meeting of the
                  Association for Computational Linguistics, {\em
                  Barcelona, Spain, 21--26 July 2004}}) 

@string( acl-04s = {Proceedings of ACL 2004})

@string( acl-05 = {Proceedings of the 43rd Annual Meeting of the
                  Association for Computational Linguistics, {\em
                  Ann Arbor, Mich., 25--30 June 2005}}) 

@string( acl-05s = {Proceedings of ACL-05})

@string( acl-07 = {Proceedings of the 45th Annual Meeting of the
                  Association for Computational Linguistics, {\em
                  Prague, Czech Republic, 23--30 June 2007}}) 

@string( acl-07s = {Proceedings of ACL-07})

 



@string( acl-comp-07 = {Companion Volume to the Proceedings of the
                  45th Annual Meeting of the Association for
                  Computational Linguistics, {\em Prague, Czech Republic,
                  23--30 June 2007}})

@string( acl-comp-07s = {Comp. Vol. to Proceedings of ACL-07})

@string( acl-08 = {Proceedings of the 46th Annual Meeting of the
                  Association for Computational Linguistics: Human
                  Language Technologies, {\em Columbus, Ohio, 15--20
                  June 2008}}) 

@string( acl-08s = {Proceedings of ACL 2008})

@string( acl-09 = {Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and
               the 4th International Joint Conference on Natural Language 
               Processing of the Asian Federation of Natural Language
               Processing (IJCNLP)}) 

@string( acl-10 = {Proceedings of the 48th Annual 
               Meeting of the Association for Computational Linguistics (ACL)}) 

@string( acl-09s = {Proceedings of ACL-IJCNLP-09})

@string( acl-10s = {Proceedings of ACL 2010})

@string( alta-09s = {Proceedings of ALTA 2009})

@string( eacl-06 = {Proceedings of the 11th Conference of the European
                  Chapter of the Association for Computational
                  Linguistics (EACL), Trento, Italy})

@string( eacl-06s = {Proceedings of EACL 2006})

@string( eacl-09 = {Proceedings of the 12th Conference of the European
                  Chapter of the Association for Computational
                  Linguistics (EACL), Athens, Greece})

@string( eacl-09s = {Proceedings of EACL-09})

@string( aaai-05 = {Proceedings of the 20th National Conference on
                  Artificial Intelligence, Pittsburgh, USA})

@string( aaai-05s = {Proceedings of AAAI-05})

@string( aaai-06 = {Proceedings of the 21st National
                   Conference on Artificial Intelligence, {\em Boston,
                   Mass., 16--20 July 2006}})

@string( aaai-06s = {Proceedings of AAAI-06})

@string( aaai-07 = {Proceedings of the 22nd Conference on the
                   Advancement of Artificial Intelligence, {\em
                   Vancouver, B.C., Canada, 22--26 July 2007}})

@string( aaai-07s = {Proceedings of AAAI-07})

@string( aaai-08 = {Proceedings of the 23rd Conference on the
                   Advancement of Artificial Intelligence, {\em
                   Chicago, Ill., 13--17 July 2008}})

@string( aaai-08s = {Proc. of AAAI-08})
@string( aaai-10s = {Proc. of AAAI 2010})

@string( coling-92 = {Proceedings of the 15th International
                  Conference on Computational Linguistics, {\em Nantes,
                  France, 23-28 August 1992}})

@string( coling-92s = {Proc. of COLING 1992})

@string( coling-04 = {Proceedings of the 20th International
                  Conference on Computational Linguistics, {\em
                  Geneva, Switzerland, 23--27 August 2004}}) 

@string( coling-04s = {Proc. of COLING-04})
@string( coling-02s = {Proc. of COLING 2002})

@string( coling-06 = {Proceedings of the 21st International
                  Conference on Computational Linguistics and
                  44th Annual Meeting of the Association for
                  Computational Linguistics (COLING-ACL), Sydney, Australia})

@string( coling-06s = {Proceedings of COLING-ACL 2006})

@string( coling-08 = {Proceedings of the 22nd International
                  Conference on Computational Linguistics, {\em
                  Manchester, U.K., 18--22 August 2008}}) 

@string( coling-08s = {Proc. of COLING 2008})
@string( coling-10s = {Proc. of COLING 2010})
@string{ cicling-09 = {Computational Linguistics and Intelligent Text
                  Processing, 10th International Conference, {\em
                  Mexico City, Mexico, 1--7 Marcg 2009}}} 

@string{ cicling-09s = {Proc. of CICLing-09}}

@string( cikm-07 = {Proceedings of the Sixteenth ACM Conference on 
                     Information and Knowledge management, Lisbon, Portugal})

@string( cikm-07s = {Proc. of CIKM-07})

@string( cikm-08 = {Proceedings of the Seventeenth ACM Conference on 
                     Information and Knowledge management {\em
                     Napa Valley, Cal., 26--30 October 2008}})

@string( cikm-08s = {Proc. of CIKM-08})

@string( cikm-09 = {Proceedings of the Eighteenth ACM Conference on 
                     Information and Knowledge management {\em
                     Hong Kong, China, 2--6 November 2009}})

@string( cikm-09s = {Proc. of CIKM 2009})
@string( cikm-10s = {Proc. of CIKM 2010})
@string( cikm-11s = {Proc. of CIKM 2011})
@string( coling-94 = {Proceedings of the 15th International
                  Conference on Computational Linguistics, {\em Kyoto,
                  Japan, 5--9 August 1994}}) 

@string( coling-94s = {Proc. of COLING-94})

@string( coling-98 = {Proceedings of the 17th International
                  Conference on Computational Linguistics and
                  36th Annual Meeting of the Association for
                  Computational Linguistics, {\em Montr{\'e}al, Qu{\'e}bec,
                  Canada, 10--14 August 1998}}) 

@string( coling-98s = {Proc. of COLING-ACL-98})

@string( coling-02 = {Proceedings of the 19th International
                  Conference on Computational Linguistics, {\em
                  Taipei, Taiwan, 24 August -- 1 September 2002}}) 


@string( coling-comp-06 = {Proceedings of the Poster Session at the
                  21st International Conference on Computational
                  Linguistics and 44th Annual Meeting of the
                  Association for Computational Linguistics, {\em
                  Sydney, Australia, 17--21 July 2006}})

@string( coling-comp-06s = {Proc. of COLING-ACL 2006 Poster Session})

@string( conll-04 = {Proceedings of the 8th Conference on
                  Computational Natural Language Learning, {\em
                  Boston, Mass., USA, 6--7 May 2004}})  

@string( conll-04s = {Proc. of CoNLL-04})

@string( eacl-03 = {Proceedings of the 10th Conference of the European
                  Chapter of the Association for Computational
                  Linguistics, {\em Budapest, Hungary, 12--17 April 2003}})  

@string( eacl-03s = {Proc. of EACL-03})

@string( emnlp-04 = {Proceedings of the 2004 Conference on
                  Empirical Methods in Natural Language Processing,
                  {\em Barcelona, Spain, 25--26 July}})

@string( emnlp-04s = {Proc. of EMNLP-04})

@string( hlt-emnlp-05 = {Proceedings of the Human Language Technology
                  Conference and the 2005 Conference on
                  Empirical Methods in Natural Language Processing (EMNLP),
                  Vancouver, B.C., Canada})

@string( hlt-emnlp-05s = {Proc. HLT-EMNLP '05})


@string( emnlp-06 = {Proceedings of the 2006 Conference on
                  Empirical Methods in Natural Language Processing,
                  {\em Sydney, Australia, 22--23 July}})

@string( emnlp-06s = {Proc. of EMNLP-06})
@string( eswc-07s = {Proc. of ESWC 2007})
@string( esws-04s = {Proc. of ESWS 2004})

@string( flairs-05 = {Proceedings of the 18th Internationa Florida AI
                      Research Symposium Conference,
                     {\em Clearwater Beach, Flo., 15--17 May 2005}})

@string( flairs-05s = {Proc. of FLAIRS-05})


@string( fois-01 = {Proceedings of the 2nd International Conference on Formal
                    Ontology in Information Systems, {\em Ogunquit,
                    Maine, 17-19 October 2001}})

@string( fois-01s = {Proc. of FOIS-01})

@string{ gwc-02 = {Proceedings of the 1st International Global WordNet
                  Conference, {\em Mysore, India, 21--25 January
                  2002}}} 

@string{ gwc-02s = {Proc. of GWC-02}}

@string{ gwc-04 = {Proceedings of the 2nd International Global WordNet
                  Conference, {\em Brno, Czech Republic, 20--23 January
                  2004}}} 

@string{ gwc-04s = {Proc. of GWC-04}}

@string{ gwc-06 = {Proceedings of the 3rd International Global WordNet
                  Conference, {\em Jeju Island, Korea, 22--26 January
                  2006}}} 

@string{ gwc-06s = {Proc. of GWC-06}}

@string( ijcai-03 = {Proceedings of the 18th International
                  Joint Conference on Artificial Intelligence,
                  {\em Acapulco, Mexico, 9--15 August 2003}}) 

@string( ijcai-03s = {Proc. of IJCAI-03})

@string( ijcai-07 = {Proceedings of the 20th International
                  Joint Conference on Artificial Intelligence,
                  {\em Hyderabad, India, 6--12 January 2007}}) 

@string( ijcai-07s = {Proc. of IJCAI 2007})

@string( ijcai-09 = {Proceedings of the 21st International
                  Joint Conference on Artificial Intelligence (IJCAI),
                  Pasadena, California, USA}) 

@string( ijcai-09s = {Proc. of IJCAI 2009})
@string( ijcai-11s = {Proc. of IJCAI 2011})
@string( ijcnlp-11s = {Proc. of IJCNLP 2011})

@string( lrec-02 = {Proceedings of the 3rd International
                  Conference on Language Resources and Evaluation,
                  {\em Las Palmas, Canary Islands, Spain, 29--31 May
                  2002}}) 

@string( lrec-02s = {Proc. of LREC '02})

@string( lrec-04 = {Proceedings of the 4th International
                  Conference on Language Resources and Evaluation,
                  {\em Lisbon, Portugal, 26--28 May 2004}}) 

@string( lrec-04s = {Proc. of LREC '04})

@string( lrec-06 = {Proceedings of the 5th International
                  Conference on Language Resources and Evaluation,
                  {\em Genoa, Italy, 22--28 May 2006}})

@string( lrec-06s = {Proc. of LREC '06})

@string( lrec-08 = {Proceedings of the 6th International
                  Conference on Language Resources and Evaluation,
                  {\em Marrakech, Morocco, 26 May -- 1 June 2008}}) 

@string( lrec-08s = {Proc. of LREC '08})
@string( lrec-10s = {Proc. of LREC 2010})

@string( naacl-07 = {Proceedings of Human Language Technologies
                  2007: The Conference of the North American Chapter of the
                  Association for Computational Linguistics, Rochester, N.Y.})

@string( naacl-07s = {Proc. of NAACL-HLT-07})

@string( naacl-09 = {Proceedings of Human Language Technologies
                  2009: The Conference of the North American Chapter of the
                  Association for Computational Linguistics, {\em
                  Boulder, Col., 31 May -- 5 June 2009}})

@string( naacl-09s = {Proc. of NAACL-HLT-09})

@string( naacl-10 = {Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics})
@string( naacl-10s = {Proc. of NAACL-HLT 2010})
@string( nips-2004s = {Proc. of NIPS 2004})
@string( nldb-05s = {Proc. of NLDB 2005})

@string( ecai-00s = {Proc. of ECAI 2000})

@string( ecai-08 = {Proceedings of the 18th European Conference on
                    Artificial Intelligence, {\em Patras, Greece,
                    21--25 July 2008}})

@string( ecai-08s = {Proc. of ECAI-08})

@string( eswc-08 = {Proceedings of the 5th European
                  Semantic Web Conference, {\em Tenerife, Spain, 1--5
                  June 2008}}) 

@string( eswc-08s = {Proc. of ESWC-08})

@string( emnlp-07 = {Proceedings of the 2007 Joint Conference on
                  Empirical Methods in Natural Language Processing and
                  Computational Language Learning, {\em Prague, Czech
                  Republic, 28--30 June}}) 

@string( emnlp-07s = {Proc. of EMNLP-CoNLL-07})

@string( emnlp-08 = {Proceedings of the 2008 Conference on
                  Empirical Methods in Natural Language Processing,
                  {\em Waikiki, Honolulu, Hawaii, 25-27 October}})

@string( emnlp-08s = {Proc. of EMNLP-08})

@string( emnlp-09 = {Proceedings of the 2009 Conference on
                  Empirical Methods in Natural Language Processing,
                  {\em Singapore, 6--7 July 2009}})

@string( emnlp-09s = {Proc. of EMNLP-09})

@string( eswc-08 = {Proceedings of the 5th European
                  Semantic Web Conference, {\em Tenerife, Spain, 1--5
                  June 2008}})

@string( eswc-08s = {Proc. of ESWC-08})

@string( iswc-07 = { Proceedings of the 6th International Semantic
                     Web Conference and 2nd Asian Semantic Web
                     Conference, {\em Busan, Korea, November 11--15 2007}})

@string( iswc-06s = {Proc. of ISWC 2006})
@string( iswc-07s = {Proc. of ISWC 2007 + ASWC 2007})

@string( ranlp-03 = {Proceedings of the International Conference on
                  Recent Advances in Natural Language Processing,
                  {\em Borovets, Bulgaria, 10--12 September 2003}}) 

@string( ranlp-03s = {Proc. of RANLP 2003})

@string( www-04 = {Proceedings of the 13th World Wide Web Conference, {\em
                  New York, N.Y., 17--22 May 2004}})

@string( www-04s = {Proc. of WWW 2004})

@string( www-05s = {Proc. of WWW 2005})

@string( www-07 = {Proceedings of the 16th World Wide Web Conference, {\em
                   Banff, Canada, 8--12 May 2007}})

@string( www-07s = {Proc. of WWW-07})

@string( www-08 = {Proceedings of the 17th World Wide Web Conference, {\em
                   Beijing, China, 21--25 April 2008}})

@string( www-08s = {Proc. of WWW-08})

@string( naacl-04 = {Proceedings of the Human Language Technology
                  Conference of the North American Chapter of the
                  Association for Computational Linguistics, {\em
                  Boston, Mass., 2--7 May 2004}})

@string( naacl-04s = {Proc. of HLT-NAACL-04})

@string( naacl-06 = {Proceedings of the Human Language Technology
                  Conference of the North American Chapter of the
                  Association for Computational Linguistics, {\em
                  New York, N.Y., 4--9 June 2006}})

@string( naacl-06s = {Proc. of HLT-NAACL-06})

@string( naacl-comp-06 = {Companion Volume to the Proceedings of the
                  Human Language Technology Conference of the North
                  American Chapter of the Association for
                  Computational Linguistics, New York, N.Y.}) 

@string( naacl-comp-06s = {Comp. Vol. to Proc. of HLT-NAACL-06})


@string( trec-00 = {Proceedings of the Ninth Text REtrieval
                  Conference, {\em Gaithersburg, Maryland, November
                  13-16 2000}})  

@string( trec-00s = {Proc. of TREC-9})

@string{ ws-senseval-2 = {Proceedings of the 2nd International
                  Workshop on Evaluating Word Sense Disambiguation
                  Systems (SENSEVAL-2) at ACL-01, {\em Toulouse,
                  France, 5--6 July 2001}}} 

@string{ ws-senseval-2s = {Proc. of SENSEVAL-2}}

@string{ ws-senseval-3 = {Proceedings of the 3rd International
                  Workshop on the Evaluation of Systems for the
                  Semantic Analysis of Text (SENSEVAL-3),
                  Barcelona, Spain}}} 

@string{ ws-senseval-3s = {Proceedings of SENSEVAL-3}}

@string( ws-semeval-07 = {Proceedings of the 4th International
                  Workshop on Semantic Evaluations
                  (SemEval-2007), Prague, Czech Republic})  

@string( ws-semeval-07s = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic})

@string{ ws-wikiai-08 = { Proceedings of the Workshop on Wikipedia and
                  Artificial Intelligence: An Evolving Synergy at
                  AAAI-08, {\em Chicago, Ill., 13 July}}}

@string{ ws-wikiai-08s = { Proceedings of the Workshop on Wikipedia and
                  Artificial Intelligence: An Evolving Synergy at
                  AAAI-08}}

@string{ACL =    "Annual Meeting of the Association for Computational
		  Linguistics"}

@string{ML = "International Conference on Machine Learning"}

@inproceedings{panchenko2013recherche,
  title={Recherche et visualisation de mots s{\'e}mantiquement li{\'e}s},
  author={Panchenko, Alexander and Naets, Hubert and Brouwers, Laetitia and Romanov, Pavel and Fairon, C{\'e}drick},
  booktitle={Proceedings of the TALN-R{\'E}CITAL 2013},
  pages={747-754},
  year={2013},
  address={Les Sables d'Olonne, France}
}

@INPROCEEDINGS{panchenko12p,
  author = {Alexander Panchenko and Olga Morozova and Hubert Naets},
  title = {A semantic similarity measure based on lexico-syntactic patterns},
  booktitle = {Proceedings of KONVENS 2012},
  pages = {174--178},
  year = {2012},
  address={Vienna, Austria},
  editor = {Jeremy Jancsary},
  publisher = {\"{O}GAI},
  url = {http://www.oegai.at/konvens2012/proceedings/23_panchenko12p/},
  note = {Main track: poster presentations}
}

@inproceedings{hearst1992automatic,
  title={Automatic acquisition of hyponyms from large text corpora},
  author={Hearst, Marti A},
  booktitle={Proceedings of the 14th conference on Computational linguistics-Volume 2},
  pages={539--545},
  year={1992}
}

@inproceedings{task13semeval2016,
  title={SemEval-2016 task 13: Taxonomy Extraction Evaluation {(TExEval-2)}},
  author={Bordea, Georgeta and Lefever, Els and Buitelaar, Paul},
  booktitle={Proceedings of the 10th International Workshop on Semantic Evaluation},
  year={2016},
  address={San Diego, CA, USA}
}

@article{velardi2013ontolearn,
  title={Ontolearn reloaded: A graph-based algorithm for taxonomy induction},
  author={Velardi, Paola and Faralli, Stefano and Navigli, Roberto},
  journal={Computational Linguistics},
  volume={39},
  number={3},
  pages={665--707},
  year={2013},
  publisher={MIT Press}
}


@article{tan2015usaar,
  title={{USAAR-WLV:} Hypernym Generation with Deep Neural Nets},
  author={Tan, Liling and Gupta, Rohit and van Genabith, Josef},
  journal={SemEval-2015},
  pages={932-937},
  year={2015},
  address={Denver, CO, USA}
}

@article{ceesay2015ntnu,
  title={NTNU: An Unsupervised Knowledge Approach for Taxonomy Extraction},
  author={Ceesay, Bamfa and Hou, Wen Juan},
  journal={SemEval-2015},
  pages={938},
  year={2015},
  address={Denver, CO, USA}
}

@article{cleuziou2015qassit,
  title={{QASSIT: A} Pretopological Framework for the Automatic Construction of Lexical Taxonomies from Raw Texts},
  author={Cleuziou, Guillaume and Buscaldi, Davide and Levorato, Gael Dias Vincent and Largeron, Christine},
  journal={SemEval-2015},
  pages={955--959},
  year={2015},
  address={Denver, CO, USA}
}

@article{bordea2015semeval,
  title={Semeval-2015 task 17: Taxonomy extraction evaluation {(TExEval)}},
  author={Bordea, Georgeta and Buitelaar, Paul and Faralli, Stefano and Navigli, Roberto},
  journal={SemEval-2015},
  volume={452},
  number={465},
  pages={902-910},
  year={2015},
  address={Denver, CO, USA}
}

@article{lefever2015lt3,
  title={{LT3:} a multi-modular approach to automatic taxonomy construction},
  author={Lefever, Els},
  journal={SemEval-2015},
  pages={944--948},
  year={2015}
}


@inproceedings{mccarthy2007semeval,
  title={Semeval-2007 task 10: English lexical substitution task},
  author={McCarthy, Diana and Navigli, Roberto},
  booktitle={Proceedings of the 4th International Workshop on Semantic Evaluations},
  pages={48--53},
  year={2007}
}

@inproceedings{mccarthy2002lexical,
  title={Lexical substitution as a task for wsd evaluation},
  author={McCarthy, Diana},
  booktitle={Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions-Volume 8},
  pages={109--115},
  year={2002},
  organization={Association for Computational Linguistics}
}

@article{mccarthy2009english,
  title={The English lexical substitution task},
  author={McCarthy, Diana and Navigli, Roberto},
  journal={Language resources and evaluation},
  volume={43},
  number={2},
  pages={139--159},
  year={2009},
  publisher={Springer}
}

@inproceedings{mihalcea2010semeval,
  title={Semeval-2010 task 2: Cross-lingual lexical substitution},
  author={Mihalcea, Rada and Sinha, Ravi and McCarthy, Diana},
  booktitle={Proceedings of the 5th international workshop on semantic evaluation},
  pages={9--14},
  year={2010},
  organization={Association for Computational Linguistics}
}

@phdthesis{panchenko2013similarity,
  title={Similarity measures for semantic relation extraction},
  author={Panchenko, Alexander},
  year={2013},
  school={Universit'{e} catholique de Louvain, Louvain-la-Neuve, Belgium}
}


@inproceedings{biemann2010co,
  title={Co-occurrence cluster features for lexical substitutions in context},
  author={Biemann, Chris},
  booktitle={Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing},
  pages={55--59},
  year={2010},
  organization={Association for Computational Linguistics}
}


@article{pedersen2005maximizing,
  title={Maximizing semantic relatedness to perform word sense disambiguation},
  author={Pedersen, Ted and Banerjee, Satanjeev and Patwardhan, Siddharth},
  journal={University of Minnesota supercomputing institute research report UMSI},
  volume={25},
  pages={2005},
  year={2005}
}

@article{weaver1955translation,
  title={Translation},
  author={Weaver, Warren},
  journal={Machine translation of languages},
  volume={14},
  pages={15--23},
  year={1955},
  publisher={Cambridge: Technology Press, MIT}
}

@inproceedings{hovy2006ontonotes,
  title={OntoNotes: the 90\% solution},
  author={Hovy, Eduard and Marcus, Mitchell and Palmer, Martha and Ramshaw, Lance and Weischedel, Ralph},
  booktitle={Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers},
  pages={57--60},
  year={2006},
  organization={Association for Computational Linguistics}
}

@inproceedings{bordag2006word,
  title={Word Sense Induction: Triplet-Based Clustering and Automatic Evaluation.},
  author={Bordag, Stefan},
  booktitle={EACL},
  year={2006},
  organization={Citeseer}
}

@inproceedings{lin1998information,
  title={An information-theoretic definition of similarity.},
  author={Lin, Dekang},
  booktitle={ICML},
  volume={98},
  pages={296--304},
  year={1998}
}


@inproceedings{pantel2002discovering,
  title={Discovering word senses from text},
  author={Pantel, Patrick and Lin, Dekang},
  booktitle={Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={613--619},
  year={2002},
  organization={ACM}
}

@article{veronis2004hyperlex,
  title={Hyperlex: lexical cartography for information retrieval},
  author={V{\'e}ronis, Jean},
  journal={Computer Speech \& Language},
  volume={18},
  number={3},
  pages={223--252},
  year={2004},
  publisher={Elsevier}
}

@article{biemann2013text,
  title={{Text: Now in 2D! a framework for lexical expansion with contextual similarity}},
  author={Biemann, Chris and Riedl, Martin},
  journal={Journal of Language Modelling},
  volume={1},
  number={1},
  pages={55--95},
  year={2013}
}

@inproceedings{widdows2002graph,
  title={A graph model for unsupervised lexical acquisition},
  author={Widdows, Dominic and Dorow, Beate},
  booktitle={Proceedings of the 19th international conference on Computational linguistics-Volume 1},
  pages={1--7},
  year={2002},
  organization={Association for Computational Linguistics}
}

@incollection{pedersen2005name,
  title={Name discrimination by clustering similar contexts},
  author={Pedersen, Ted and Purandare, Amruta and Kulkarni, Anagha},
  booktitle={Computational Linguistics and Intelligent Text Processing},
  pages={226--237},
  year={2005},
  publisher={Springer}
}

@article{savova2005resolving,
  title={Resolving ambiguities in biomedical text with unsupervised clustering approaches},
  author={Savova, Guergana and Pedersen, Ted and Purandare, Amruta and Kulkarni, Anagha},
  journal={University of Minnesota Supercomputing Institute Research Report},
  year={2005}
}

@article{pedersen1997distinguishing,
  title={Distinguishing word senses in untagged text},
  author={Pedersen, Ted and Bruce, Rebecca},
  journal={ACL},
  year={1997}
}

@article{schutze1998automatic,
  title={Automatic word sense discrimination},
  author={Sch{\"u}tze, Hinrich},
  journal={Computational linguistics},
  volume={24},
  number={1},
  pages={97--123},
  year={1998},
  publisher={MIT Press}
}

@book{agirre2007word,
  title={Word sense disambiguation: Algorithms and applications},
  author={Agirre, Eneko and Edmonds, Philip Glenny},
  volume={33},
  year={2007},
  publisher={Springer Science \& Business Media}
}

@article{biemann2013creating,
  title={Creating a system for lexical substitutions from scratch using crowdsourcing},
  author={Biemann, Chris},
  journal={Language Resources and Evaluation},
  volume={47},
  number={1},
  pages={97--122},
  year={2013},
  publisher={Springer}
}

@article{camacho2015unified,
  title={A unified multilingual semantic representation of concepts},
  author={Camacho-Collados, Jos{\'e} and Pilehvar, Mohammad Taher and Navigli, Roberto},
  journal={Proceedings of ACL, Beijing, China},
  year={2015}
}




@Article{JoBim2013,
  Title                    = {Text: Now in 2{D}! A Framework for Lexical Expansion with Contextual Similarity},
  Author                   = {Chris Biemann and Martin Riedl},
  Journal                  = {Journal of Language Modelling},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {55--95},
  Volume                   = {1}
}

@InProceedings{klaussner2011lexico,
  Title                    = {Lexico-Syntactic Patterns for Automatic Ontology Building.},
  Author                   = {Klaussner, Carmen and Zhekova, Desislava},
  Booktitle                = {Student Research Workshop at RANLP 2011},
  Year                     = {2011},
  pages ={109--114},

  Owner                    = {eruppert},
  Timestamp                = {2015.04.17}
}


@Article{kluegl2014uima,
  Title                    = {{UIMA Ruta: Rapid development of rule-based information extraction applications}},
  Author                   = {Kluegl, Peter and Toepfer, Martin and Beck, Philip-Daniel and Fette, Georg and Puppe, Frank},
  Journal                  = {Natural Language Engineering},
  Year                     = {2014},
  Pages                    = {1--40},
  volume =  22,
  number = 1,

  Publisher                = {Cambridge Univ Press},
  Timestamp                = {2015.05.08}
}

@article{navigli2009word,
  title={Word sense disambiguation: A survey},
  author={Navigli, Roberto},
  journal={ACM Computing Surveys (CSUR)},
  volume={41},
  number={2},
  pages={10},
  year={2009},
  publisher={ACM}
}


phdthesis{panchenko2013similarity,
  title={Similarity measures for semantic relation extraction},
  author={Panchenko, Alexander},
  year={2013},
  school={Universit'{e} catholique de Louvain, Louvain-la-Neuve, Belgium}
}


@InProceedings{li2015multi,
  title={Do Multi-Sense Embeddings Improve Natural Language Understanding?},
  author={Li, Jiwei and Jurafsky, Dan},
  booktitle={Conference on Empirical Methods in Natural Language Processing, EMNLP'2015},
  year={2015}
}

@article{biemann2013creating,
  title={Creating a system for lexical substitutions from scratch using crowdsourcing},
  author={Biemann, Chris},
  journal={Language Resources and Evaluation},
  volume={47},
  number={1},
  pages={97--122},
  year={2013},
  publisher={Springer}
}

@article{camacho2015unified,
  title={A unified multilingual semantic representation of concepts},
  author={Camacho-Collados, Jos{\'e} and Pilehvar, Mohammad Taher and Navigli, Roberto},
  journal={Proceedings of ACL, Beijing, China},
  year={2015}
}

@InProceedings{tianEtAl2014,
  author    = {Tian, Fei  and  Dai, Hanjun  and  Bian, Jiang  and  Gao, Bin  and  Zhang, Rui  and  Chen, Enhong  and  Liu, Tie-Yan},
  title     = {A Probabilistic Model for Learning Multi-Prototype Word Embeddings},
  booktitle = {COLING},
  moth     = {August},
  year      = {2014},
  address   = {Dublin, Ireland},
  fublisher = {Dublin City University and Association for Computational Linguistics},
  pages     = {151--160},
  url       = {http://www.aclweb.org/anthology/C14-1016}
}

@inproceedings{pradhanEtAl2007,
    author = {Pradhan, Sameer and Loper, Edward and Dligach, Dmitriy and Palmer, Martha},
    title = {{SemEval-2007 Task-17: English Lexical Sample, SRL and All Words}},
    booktitle = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
    moth = {June},
    year = {2007},
    address = {Prague, Czech Republic},
    fublisher = {Association for Computational Linguistics},
    pages = {87--92},
    curl = {http://www.aclweb.org/anthology/S07-1016}
    }


@inproceedings{Hovy06,
 author = {Hovy, Eduard and Marcus, Mitchell and Palmer, Martha and Ramshaw, Lance and Weischedel, Ralph},
 title = {OntoNotes: The 90\% Solution},
 booktitle = {Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers},
 series = {NAACL-Short '06},
 year = {2006},
 address = {New York City, NY, USA},
 pages = {57--60},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=1614049.1614064},
 fublisher = {Association for Computational Linguistics},
 pubaddress = {Stroudsburg, PA, USA},
} 




@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{rothe2015autoextend,
  title={AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes},
  author={Rothe, Sascha and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1507.01127},
  year={2015}
}

@InProceedings{erkEtAl2009,
  author    = {Erk, Katrin  and  McCarthy, Diana  and  Gaylord, Nicholas},
  title     = {Investigations on Word Senses and Word Usages},
  booktitle = {ACL},
  year      = {2009},
  address   = {Suntec, Singapore},
  pages     = {10--18},
  url       = {http://www.aclweb.org/anthology/P/P09/P09-1002}
}


@inproceedings{McCarthy2004,
 author = {McCarthy, Diana and Koeling, Rob and Weeds, Julie and Carroll, John},
 title = {Finding Predominant Word Senses in Untagged Text},
 booktitle = {ACL},

 year = {2004},
 address = {Barcelona, Spain},
 blaarticleno = {279},
 blaurl = {http://dx.doi.org/10.3115/1218955.1218991},
 bladoi = {10.3115/1218955.1218991},
 fublisher = {Association for Computational Linguistics},
 pubaddress = {Stroudsburg, PA, USA},
} 

@InProceedings{agirreEtAl2006,
  author    = {Agirre, Eneko  and  Mart\'{\i}nez, David  and  L\'{o}pez de Lacalle, Oier  and  Soroa, Aitor},
  title     = {Evaluating and optimizing the parameters of an unsupervised graph-based WSD algorithm},
  booktitle = {Proceedings of TextGraphs},
  moth     = {June},
  year      = {2006},
  address   = {NY, USA},
  fublisher = {Association for Computational Linguistics},
  pages     = {89--96},
  url       = {http://www.aclweb.org/anthology/W/W06/W06-3814}
}

@inproceedings{Biemann2010,
 author = {Biemann, Chris},
 title = {Co-occurrence Cluster Features for Lexical Substitutions in Context},
 booktitle = {Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing},
 series = {TextGraphs-5},
 year = {2010},
 isbn = {978-1-932432-77-0},
 address = {Uppsala, Sweden},
 pages = {55--59},
 } 

@inproceedings{bordes2011,
  title = {Learning Structured Embeddings of Knowledge Bases},
  author = {A. Bordes and J. Weston and R. Collobert and Y. Bengio},
  booktitle = {Proceedings of AAAI},
  year = {2011},
  address = {San Francisco, CA, USA}
}


@book{Chiarcos2012,
  editor    = {Christian Chiarcos and
               Sebastian Nordhoff and
               Sebastian Hellmann},
  title     = {Linked Data in Linguistics - Representing and Connecting Language
               Data and Language Metadata},
  publisher = {Springer},
  year      = {2012},
  url       = {http://dx.doi.org/10.1007/978-3-642-28249-2},
  doi       = {10.1007/978-3-642-28249-2},
  isbn      = {978-3-642-28248-5},
  timestamp = {Tue, 03 Sep 2013 15:10:08 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/books/daglib/0028690},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


 @article{Kilgarriff2014, title={The Sketch Engine: Ten years on}, volume={1}, DOI={10.1007/s40607-014-0009-9}, number={1}, journal={Lexicography}, author={Kilgarriff, Adam and Baisa, V\'{i}t and Bu\v{s}ta, Jan and Jakub\'{i}\v{c}ek, Milo\v{s} and Kov\'{a}\v{r}, Vojt\v{e}ch and Michelfeit, Jan and Rychl\'{y}, Pavel and Suchomel ,V\'{i}t}, year={2014}, moth={Jul}, pages={7--36}}

 @book{Hanks2013, place={Cambridge, Mass}, title={Lexical analysis: norms and exploitations}, publisher={The MIT Press}, author={Hanks, Patrick}, year={2013}}


@inproceedings{Huang2012,
 author = {Huang, Eric H. and Socher, Richard and Manning, Christopher D. and Ng, Andrew Y.},
 title = {Improving Word Representations via Global Context and Multiple Word Prototypes},
 booktitle = {ACL},
 year = {2012},
 address = {Jeju Island, Korea},
 pages = {873--882},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=2390524.2390645},
 acmid = {2390645},
 fublisher = {Association for Computational Linguistics},
 paddress = {Stroudsburg, PA, USA},
} 




@book{AgirreAndEdmonds2006,
	citeulike-article-id = {1152725},
	editor = {Agirre, Eneko   and Edmonds, Philip },
	howpublished = {Hardcover},
	isbn = {1402048084},
	moth = {July},
	publisher = {Springer},
	series = {Text, Speech and Language Technology},
	title = {{Word Sense Disambiguation: Algorithms and Applications}},
	year = {2006}
}

@InProceedings{banerjee2002adapted,
  author = 	 {Satanjeev Banerjee and Ted Pedersen},
  title = 	 {An adapted {L}esk algorithm for word sense disambiguation using {W}ord{N}et},
  booktitle = 	 {Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics},
  year = 	 2002,
  address = {Mexico City, Mexico}
}

@inproceedings{MillerEtAl2012,
	author = {Tristan Miller and Chris Biemann and Torsten Zesch and Iryna Gurevych},
	title = {Using Distributional Similarity for Lexical Expansion in Knowledge based Word Sense Disambiguation},
	month = dec,
	year = {2012},
	booktitle = {Proceedings of the 24th International Conference on Computational
Linguistics (COLING 2012)},
	pages = {1781-1796},
	address = {Mumbai, India},
	    url = {http://aclweb.org/anthology/C/C12/C12-1109.pdf}
}



@article{baroni2010distributional,
  title={Distributional memory: A general framework for corpus-based semantics},
  author={Baroni, Marco and Lenci, Alessandro},
  journal={Computational Linguistics},
  volume={36},
  number={4},
  pages={673--721},
  year={2010},
  publisher={MIT Press}
}

@inproceedings{gurevych2012uby,
  title={Uby: A large-scale unified lexical-semantic resource based on LMF},
  author={Gurevych, Iryna and Eckle-Kohler, Judith and Hartmann, Silvana and Matuschek, Michael and Meyer, Christian M and Wirth, Christian},
  booktitle={Proceedings of EACL},
  pages={580--590},
  year={2012},
  address = {Avignon, France},
  forganization={Association for Computational Linguistics}
}



@article{everett2005ego,
  title={Ego network betweenness},
  author={Everett, Martin and Borgatti, Stephen P},
  journal={Social networks},
  volume={27},
  number={1},
  pages={31--38},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{neelakantanefficient,
  title={Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space},
  author={Neelakantan, Arvind and Shankar, Jeevan and Passos, Alexandre and McCallum, Andrew},
  booktitle={EMNLP},
  year={2014},
  address={Doha, Qatar}
}

@article{bartunov2015breaking,
  title={Breaking Sticks and Ambiguities with Adaptive Skip-gram},
  author={Bartunov, Sergey and Kondrashkin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:1502.07257},
  year={2015}
}

@incollection{panchenko2013serelex,
  title={Serelex: Search and visualization of semantically related words},
  author={Panchenko, Alexander and Romanov, Pavel and Morozova, Olga and Naets, Hubert and Philippovich, Andrey and Romanov, Alexey and Fairon, C{\'e}drick},
  booktitle={Advances in Information Retrieval},
  pages={837--840},
  year={2013},
  publisher={Springer}
}


@book{biemann2011structure,
  title={Structure discovery in natural language},
  author={Biemann, Chris and van den Bosch, Antal},
  year={2011},
  publisher={Springer Science \& Business Media}
}



@article{everett2005ego,
  title={Ego network betweenness},
  author={Everett, Martin and Borgatti, Stephen P},
  journal={Social networks},
  volume={27},
  number={1},
  pages={31--38},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{neelakantanefficient,
  title={Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space},
  author={Neelakantan, Arvind and Shankar, Jeevan and Passos, Alexandre and McCallum, Andrew},
  booktitle={In Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2014},
  address={Doha, Qatar}
}

@article{bartunov2015breaking,
  title={Breaking Sticks and Ambiguities with Adaptive Skip-gram},
  author={Bartunov, Sergey and Kondrashkin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:1502.07257},
  year={2015}
}

@incollection{panchenko2013serelex,
  title={Serelex: Search and visualization of semantically related words},
  author={Panchenko, Alexander and Romanov, Pavel and Morozova, Olga and Naets, Hubert and Philippovich, Andrey and Romanov, Alexey and Fairon, C{\'e}drick},
  booktitle={Advances in Information Retrieval},
  pages={837--840},
  year={2013},
  publisher={Springer}
}


@book{biemann2011structure,
  title={Structure discovery in natural language},
  author={Biemann, Chris},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@inproceedings{biemann2006chinese,
  title={Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems},
  author={Biemann, Chris},
  booktitle={Proceedings of the first workshop on graph based methods for natural language processing},
  pages={73--80},
  year={2006},
  address = {New York City, NY, USA}
}

@article{moro2014entity,
  title={Entity linking meets word sense disambiguation: a unified approach},
  author={Moro, Andrea and Raganato, Alessandro and Navigli, Roberto},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={231--244},
  year={2014}
}

@InProceedings{mitra-EtAl:2014:P14-1,
  author    = {Mitra, Sunny  and  Mitra, Ritwik  and  Riedl, Martin  and  Biemann, Chris  and  Mukherjee, Animesh  and  Goyal, Pawan},
  title     = {That's sick dude!: Automatic identification of word sense change across different timescales},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland},
  
  pages     = {1020--1029},
  url       = {http://www.aclweb.org/anthology/P14-1096}
}

@article{panchenko2012study,
  title={A study of heterogeneous similarity measures for semantic relation extraction},
  author={Panchenko, Alexander},
  journal={Proceedings of the JEP-TALN-RECITAL},
  volume={3},
  pages={29--42},
  year={2012},
  address={Grenoble, France}
}

@article{nivre2007maltparser,
  title={MaltParser: A language-independent system for data-driven dependency parsing},
  author={Nivre, Joakim and Hall, Johan and Nilsson, Jens and Chanev, Atanas and Eryigit, G{\"u}lsen and K{\"u}bler, Sandra and Marinov, Svetoslav and Marsi, Erwin},
  journal={Natural Language Engineering},
  volume={13},
  number={02},
  pages={95--135},
  year={2007},
  publisher={Cambridge Univ Press}
}


@inproceedings{Mooney1996,
abstract = {This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word ``line'' using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference. We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems.},
address = {Philadelphia, PA},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9612001},
author = {Mooney, Raymond J.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
eprint = {9612001},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/9612001.pdf:pdf},
pages = {82--91},
primaryClass = {cmp-lg},
title = {{Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning}},
url = {http://arxiv.org/abs/cmp-lg/9612001},
year = {1996}
}
@phdthesis{Pedersen1998,
abstract = {This dissertation presents several new methods of supervised and unsupervised learning of word sense disambiguation models. The supervised methods focus on performing model searches through a space of probabilistic models, and the unsupervised methods rely on the use of Gibbs Sampling and the Expectation Maximization (EM) algorithm. In both the supervised and unsupervised case, the Naive Bayesian model is found to perform well. An explanation for this success is presented in terms of learning rates and bias-variance decompositions.},
archivePrefix = {arXiv},
arxivId = {arXiv:0707.3972v1},
author = {Pedersen, Ted},
eprint = {arXiv:0707.3972v1},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/0707.3972.pdf:pdf},
school = {Southern Methodist University, Dallas, TX},
title = {{Learning Probabilistic Models of Word Sense Disambiguation}},
url = {http://arxiv.org/abs/0707.3972},
year = {2007}
}
@inproceedings{Steinbach2000,
address = {Boston, MA, USA},
author = {Steinbach, Michael and Karypis, George and Kumar, Vipin and Others},
booktitle = {KDD workshop on text mining},
doi = {10.1.1.125.9225},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Steinbach2000.pdf:pdf},
keywords = {document clustering},
number = {1},
pages = {525--526},
title = {{A comparison of document clustering techniques}},
volume = {400},
year = {2000}
}
@inproceedings{Pradhan2007,
abstract = {The OntoNotes project is creating a corpus of large-scale, accurate, and integrated annotation of multiple levels of the shallow semantic structure in text. Such rich, integrated annotation covering many levels will allow for richer, cross-level models enabling significantly better automatic semantic analysis. At the same time, it demands a robust, efficient, scalable mechanism for storing and accessing these complex inter-dependent annotations. We describe a relational database representation that captures both the inter- and intra-layer dependencies and provide details of an object-oriented API for efficient, multi-tiered access to this data.},
address = {Irvine, CA},
author = {Pradhan, Sameer S. and Hovy, Eduard and Marcus, Mitch and Palmer, Martha and Ramshaw, Lance and Weischedel, Ralph},
booktitle = {ICSC 2007 International Conference on Semantic Computing},
doi = {10.1109/ICSC.2007.83},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/07ICSC-OntoNotes-database.pdf:pdf},
isbn = {0769529976},
issn = {1793-351X},
pages = {517--524},
title = {{OntoNotes: A unified relational semantic representation}},
year = {2007}
}
@inproceedings{Ng1997,
abstract = {In this paper, we report recent improvements to the exemplar-based learning approach for word sense disambiguation that have achieved higher disambiguation accuracy. By using a larger value of \$k\$, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best \$k\$, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in $\backslash$cite\{ng96\}. The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in $\backslash$cite\{mooney96\} to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms.},
address = {Providence, USA},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9706010},
author = {Ng, Hwee Tou},
booktitle = {Proceedings of the Second Conference on Empirical Methods in Natural Language Processing},
eprint = {9706010},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/9706010.pdf:pdf},
pages = {208--213},
primaryClass = {cmp-lg},
title = {{Exemplar-Based Word Sense Disambiguation: Some Recent Improvements}},
url = {http://arxiv.org/abs/cmp-lg/9706010},
year = {1997}
}
@article{Moro2014,
abstract = {Entity Linking (EL) and Word Sense Disam- biguation (WSD) both address the lexical am- biguity of language. But while the two tasks are pretty similar, they differ in a fundamen- tal respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while inWSDthere is a perfect match between theword form (bet- ter, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate mean- ings coupled with a densest subgraph heuris- tic which selects high-coherence semantic in- terpretations. Our experiments show state-of- the-art performances on both tasks on 6 differ- ent datasets, including a multilingual setting. Babelfy is online at http://babelfy.org 1},
author = {Moro, Andrea and Raganato, Alessandro and Navigli, Roberto and Informatica, Dipartimento and Elena, Viale Regina},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/EntityLinkingMeetsWSDMoro2014.pdf:pdf},
journal = {Transactions of the Association for Computational Linguistics},
pages = {231--244},
title = {{Entity Linking meets Word Sense Disambiguation : a Unified Approach}},
volume = {2},
year = {2014}
}
@inproceedings{Shafer2010,
abstract = {Hadoop is a popular open-source implementation of MapReduce for the analysis of large datasets. To manage storage resources across the cluster, Hadoop uses a distributed user-level filesystem. This filesystem - HDFS - is written in Java and designed for portability across heterogeneous hardware and software platforms. This paper analyzes the performance of HDFS and uncovers several performance issues. First, architectural bottlenecks exist in the Hadoop implementation that result in inefficient HDFS usage due to delays in scheduling new MapReduce tasks. Second, portability limitations prevent the Java implementation from exploiting features of the native platform. Third, HDFS implicitly makes portability assumptions about how the native platform manages storage resources, even though native filesystems and I/O schedulers vary widely in design and behavior. This paper investigates the root causes of these performance bottlenecks in order to evaluate tradeoffs between portability and performance in the Hadoop distributed filesystem.},
address = {White Plains, NY},
author = {Shafer, Jeffrey and Rixner, Scott and Cox, Alan L.},
booktitle = {ISPASS 2010 - IEEE International Symposium on Performance Analysis of Systems and Software},
doi = {10.1109/ISPASS.2010.5452045},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/HdfsShafer2010.pdf:pdf},
isbn = {9781424460229},
pages = {122--133},
title = {{The Hadoop distributed filesystem: Balancing portability and performance}},
year = {2010}
}
@book{Kucera1967,
address = {Providence, RI},
author = {Ku\v{c}era, H and Francis, W N},
publisher = {Brown University Press},
title = {{Computational Analysis of Present-Day American English}},
year = {1967}
}
@inproceedings{Biemann2006,
abstract = {We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges. After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation. At this, the fact is employed that the small-world property holds for many graphs in NLP.},
address = {New York City, USA},
author = {Biemann, Chris},
booktitle = {Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing},
doi = {10.3115/1654758.1654774},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/BiemannTextGraph06.pdf:pdf},
pages = {73--80},
title = {{Chinese Whispers: An Efficient Graph Clustering Algorithm and Its Application to Natural Language Processing Problems}},
url = {http://dl.acm.org/citation.cfm?id=1654774},
year = {2006}
}
@article{Hakimov2013,
address = {New York, New York, USA},
author = {Hakimov, Sherzod and Tunc, Hakan and Akimaliev, Marlen and Dogdu, Erdogan},
doi = {10.1145/2457317.2457331},
file = {:Volumes/Bill/Documents/Uni/Watson-Projekt/a12-hakimov.pdf:pdf},
isbn = {9781450315999},
journal = {Proceedings of the Joint EDBT/ICDT 2013 Workshops on - EDBT '13},
keywords = {linked data,pattern extraction,question answering,semantic},
pages = {83},
publisher = {ACM Press},
title = {{Semantic question answering system over linked data using relational patterns}},
url = {http://dl.acm.org/citation.cfm?doid=2457317.2457331},
year = {2013}
}
@article{Florian2002,
abstract = {Classifier combination is an effective and broadly useful method of improving system performance. This article investigates in depth a large number of both well-established and novel classifier combination approaches for the word sense disambiguation task, studied over a diverse classifier pool which includes feature-enhanced Na\"{\i}ve Bayes, Cosine, Decision List, Transformation-based Learning and MMVC classifiers. Each classifier has access to the same rich feature space, comprised of distance weighted bag-of-lemmas, local ngram context and specific syntactic relations, such as Verb-Object and Noun-Modifier. This study examines several key issues in system combination for the word sense disambiguation task, ranging from algorithmic structure to parameter estimation. Experiments using the standard SENSEVAL2 lexical-sample data sets in four languages (English, Spanish, Swedish and Basque) demonstrate that the combination system obtains a significantly lower error rate when compared with other systems participating in the SENSEVAL2 exercise, yielding state-of-the-art performance on these data sets.},
author = {Florian, Radu and Cucerzan, Silviu and Schafer, Charles and Yarowsky, David},
doi = {10.1017/S1351324902002978},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Florian2002.pdf:pdf},
issn = {13513249},
journal = {Natural Language Engineering},
number = {4},
pages = {327--341},
title = {{Combining Classifiers for word sense disambiguation}},
volume = {8},
year = {2002}
}
@book{Duda1973,
abstract = {Classic book on pattern recognition. Interesting points: 1) p. 66, and p. 114: Mentions the problems with dimensionality curse. 2) p. 243-246: Mentions Multidimensional scaling (MDS), Karhunen-Loeve and dimensionality reduction. Also, has the spiral data-set as a sample. 3) p. 333: mentions SVD/eigenvalues for linear fitting.},
author = {Duda, R O and Hart, P E},
booktitle = {Leonardo},
doi = {10.2307/1573081},
isbn = {0471223611},
issn = {0024094X},
pages = {482},
pmid = {16337619},
title = {{Pattern Classification and Scene Analysis}},
url = {http://www.jstor.org/stable/1573081?origin=crossref},
volume = {7},
year = {1973}
}
@article{Pedersen2006,
author = {Pedersen, Ted and Kulkarni, Anagha},
doi = {10.3115/1608974.1608983},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Pedersen2006.pdf:pdf},
isbn = {1-932432-59-0},
journal = {Proceedings of the Eleventh Conference of the European Chapter of the ACL: Posters \& Demonstrations on - EACL '06},
pages = {111--114},
title = {{Selecting the "right" number of senses based on clustering criterion functions}},
url = {http://portal.acm.org/citation.cfm?doid=1608974.1608983},
year = {2006}
}
@inproceedings{Schutze1992,
abstract = {The representation of documents and queries as vectors in a high-dimensional space is well-established in information retrieval. The author proposes that the semantics of words and contexts in a text be represented as vectors. The dimensions of the space are words and the initial vectors are determined by the words occurring close to the entity to be represented, which implies that the space has several thousand dimensions (words). This makes the vector representations (which are dense) too cumbersome to use directly. Therefore, dimensionality reduction by means of a singular value decomposition is employed. The author analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction},
address = {Minneapolis, Minnesota, USA},
author = {Sch\"{u}tze, H.},
booktitle = {Proceedings of the 1992 ACM/IEEE conference on Supercomputing},
doi = {10.1109/SUPERC.1992.236684},
isbn = {0-8186-2630-5},
publisher = {IEEE Computer Society Press},
title = {{Dimensions of meaning}},
year = {1992}
}
@inproceedings{Tratz2007,
address = {Prague, Czech Republic},
author = {Tratz, Stephen and Sanfilippo, Antonio and Gregory, Michelle and Chappell, Alan and Whitney, Paul},
booktitle = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Tratz et al. - 2007 - PNNL A Supervised Maximum Entropy Approach to Word Sense Disambiguation.pdf:pdf},
pages = {264--267},
title = {{PNNL : A Supervised Maximum Entropy Approach to Word Sense Disambiguation}},
year = {2007}
}
@inproceedings{Veronis1998,
abstract = {This paper describes two experiments on polysemy judgement and sense annotation. The first experiment enabled us to select the most polysemous words which were used in the second experiment, and which serve as test words for the evaluation of WSD systems. We show that this selection method yields results different from selecting words on the basis of their number of senses in a dictionary, and is more appropriate in the context. Both experiments show considerable disagreement among the human judges. Disagreement on sense annotation is particularly concerning, since it sheds some doubt on the very possibility of evaluating WSD systems. However, we show that a lot of the disagreement is due to the too fine granularity of sense definitions in dictionaries. We also show that clustering techniques can enable us to re-code the individual annotations according to a small set of "super-tags", and obtain a quite satisfactory level of inter-annotator agreement. Surprisingly enough, the natural clustering provided by lexicographers in the hierarchy of dictionary entries does not provide a substantial disagreement reduction, whereas "blind" data-oriented techniques produce satisfactory results. Beyond its practical goals, this paper therefore probably raises some more general questions about lexicographic practice and the adequacy of dictionaries to NLP tasks.},
address = {Herstmonceux Castle (England)},
author = {V\'{e}ronis, Jean},
booktitle = {Programme and advanced papers of the Senseval workshop},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Veronis1998.pdf:pdf},
title = {{A study of polysemy judgements and inter-annotator agreement}},
year = {1998}
}
@inproceedings{Hope2013a,
address = {Atlanta, Georgia, USA},
author = {Hope, David and Keller, Bill},
booktitle = {Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/UoSHope2013.pdf:pdf},
number = {1},
pages = {689--694},
title = {{UoS: A Graph-Based System for Graded Word Sense Induction}},
url = {http://www.aclweb.org/anthology/S13-2113},
year = {2013}
}
@article{Fundel2007,
abstract = {MOTIVATION: The discovery of regulatory pathways, signal cascades, metabolic processes or disease models requires knowledge on individual relations like e.g. physical or regulatory interactions between genes and proteins. Most interactions mentioned in the free text of biomedical publications are not yet contained in structured databases. RESULTS: We developed RelEx, an approach for relation extraction from free text. It is based on natural language preprocessing producing dependency parse trees and applying a small number of simple rules to these trees. We applied RelEx on a comprehensive set of one million MEDLINE abstracts dealing with gene and protein relations and extracted approximately 150,000 relations with an estimated performance of both 80\% precision and 80\% recall. AVAILABILITY: The used natural language preprocessing tools are free for use for academic research. Test sets and relation term lists are available from our website (http://www.bio.ifi.lmu.de/publications/RelEx/).},
author = {Fundel, Katrin and K\"{u}ffner, Robert and Zimmer, Ralf},
journal = {Bioinformatics},
number = {3},
pages = {365--371},
pmid = {17142812},
title = {{RelEx - Relation extraction using dependency parse trees}},
volume = {23},
year = {2007}
}
@inproceedings{Schutze1995,
abstract = {This paper proposes an algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co-occurrence. It differs from standard approaches by allowing for as fine grained distinctions as is warranted by the information at hand, rather than supposing a fixed number of senses per word, and by allowing for more than one sense to be assigned to a given word occurrence. The algorithm is applied to the standard vectorspace information retrieval model...},
address = {Las Vegas, NV USA},
author = {Sch\"{u}tze, Hinrich and Pedersen, Jan O},
booktitle = {Fourth Annual Symposium on Document Analysis and Information Retrieval},
pages = {161--175},
title = {{Information Retrieval Based on Word Senses}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.6617},
year = {1995}
}
@inproceedings{Pradhan2009,
address = {New York City, USA},
author = {Pradhan, Sameer S and Xue, Nianwen},
booktitle = {Proceedings of the Human Language Technology Conference of the NAACL},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Pradhan, Xue - 2009 - OntoNotes The 90 \% Solution.pdf:pdf},
pages = {57--60},
title = {{OntoNotes: The 90\% Solution}},
year = {2009}
}
@article{TaxiNY,
author = {Bloch, Matthew and Fessenden, Ford and Carter, Shan},
journal = {The New York Times},
title = {{New York Taxi Flow Heatmap, http://www.nytimes.com/interactive/2010/04/02/nyregion/taxi-map.html}},
url = {http://www.nytimes.com/interactive/2010/04/02/nyregion/taxi-map.html},
year = {2010}
}
@inproceedings{Chklovski2002,
abstract = {Open Mind Word Expert is an imple- mented active learning system for col- lecting word sense tagging from the general public over theWeb. It is avail- able at http://teach-computers.org. We expect the system to yield a large vol- ume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sam- ple activity where the training data is collected via Open Mind Word Expert. If successful, the collection process can be extended to create the definitive cor- pus of word sense information.},
address = {Philadelphia, PA},
author = {Chklovski, Timothy and Mihalcea, Rada},
booktitle = {Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions},
doi = {10.3115/1118675.1118692},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/chklovski.aclwsd02.pdf:pdf},
pages = {116--122},
title = {{Building a Sense Tagged Corpus with Open Mind Word Expert}},
year = {2002}
}
@misc{CoalDiver,
author = {{Plains Justice}},
title = {{Coal Diver, http://www.coaldiver.org/menu/heatmap}},
url = {http://www.coaldiver.org/menu/heatmap},
urldate = {2011-01-01}
}
@article{Miller1991,
abstract = {The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be. },
author = {Miller, George A. and Charles, Walter G.},
doi = {10.1080/01690969108406936},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Miller1991.pdf:pdf},
issn = {0169-0965},
journal = {Language and Cognitive Processes},
number = {1},
pages = {1--28},
title = {{Contextual correlates of semantic similarity}},
volume = {6},
year = {1991}
}
@inproceedings{Aoki2009,
abstract = {Researchers are developing mobile sensing platforms to facilitate public awareness of environmental conditions. However, turning such awareness into practical community action and political change requires more than just collecting and presenting data. To inform research on mobile environmental sensing, we conducted design fieldwork with government, private, and public interest stakeholders. In parallel, we built an environmental air quality sensing system and deployed it on street sweeping vehicles in a major U.S. city; this served as a ‚Äúresearch vehicle‚Äù by grounding our interviews and affording us status as environmental action researchers. In this paper, we present a qualitative analysis of the landscape of environmental action, focusing on insights that will help researchers frame meaningful technological interventions.},
author = {Aoki, P.M. and Honicky, RJ and Mainwaring, Alan and Myers, Chris and Paulos, Eric and Subramanian, Sushmita and Woodruff, Allison},
booktitle = {Proceedings of the 27th international conference on Human factors in computing systems},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Aoki et al. - 2009 - A vehicle for research using street sweepers to explore the landscape of environmental community action.pdf:pdf},
isbn = {9781605582467},
keywords = {map-based visualization},
mendeley-tags = {map-based visualization},
pages = {375--384},
publisher = {ACM},
title = {{A vehicle for research: using street sweepers to explore the landscape of environmental community action}},
url = {http://dl.acm.org/citation.cfm?id=1518762},
year = {2009}
}
@inproceedings{Matuszek2006,
abstract = {From the beginning, a primary goal of the Cyc project has been to build a large knowledge base containing a store of formalized background knowledge suitable for supporting reasoning in a variety of domains. In this paper, we will discuss the portion of Cyc technology that has been released in open source form as OpenCyc, provide examples of the content available in ResearchCyc, and discuss their utility for the future development of fully formalized knowledge bases. Introduction},
address = {Stanford, CA},
author = {Matuszek, Cynthia and Cabral, John and Witbrock, Michael and Deoliveira, John},
booktitle = {Proceedings of the 2006 AAAI Spring Symposium on Formalizing and Compiling Background Knowledge and Its Applications to Knowledge Representation and Question Answering},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/AAAI06SS-SyntaxAndContentOfCyc.pdf:pdf},
isbn = {1577352661},
number = {1447},
pages = {44--49},
title = {{An introduction to the syntax and content of Cyc}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.1357\&amp;rep=rep1\&amp;type=pdf},
volume = {3864},
year = {2006}
}
@inproceedings{Pianta2002,
abstract = {This paper illustrates the MultiWordNet project, aimed at producing an Italian WordNet strongly$\backslash$naligned with the Princeton WordNet. The main conceptual differences between the MultiWordNet and$\backslash$nthe EuroWordNet conceptual models are presented first. Then two automatic procedures capable of$\backslash$nspeeding up the work of lexicographers are described. Finally, we give some details about the adopted$\backslash$ndata model and we present a graphical user interface that can be used to browse and update the$\backslash$naligned database.$\backslash$n},
address = {Mysore, India},
author = {Pianta, Emanuele and Bentivogli, Luisa and Girardi, Christian},
booktitle = {Proceedings of the First International Conference on Global WordNet},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/MWN-India-published.pdf:pdf},
pages = {21--25},
title = {{MultiWordNet: developing an aligned multilingual database}},
url = {http://multiwordnet.fbk.eu/paper/MWN-India-published.pdf},
year = {2002}
}
@inproceedings{Lau2012,
abstract = {We apply topic modelling to automatically induce word senses of a target word, and demonstrate that our word sense induction method can be used to automatically detect words with emergent novel senses, as well as token occurrences of those senses. We start by exploring the utility of standard topic models for word sense induction (WSI), with a pre- determined number of topics (= senses). We next demonstrate that a non-parametric formulation that learns an appropriate number of senses per word actually performs better ...},
address = {Avignon, France},
author = {Lau, Jey Han and Cook, Paul and McCarthy, Diana and Newman, David and Baldwin, Timothy and Computing, Lexical},
booktitle = {Proceedings of the 13th Conference of the European Chapter of the ACL (EACL 2012)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/WSIForNovelSenseDetectionLau2012.pdf:pdf},
isbn = {9781937284190},
pages = {591--601},
title = {{Word Sense Induction for Novel Sense Detection}},
year = {2012}
}
@article{Bruce1999,
abstract = {In this paper, we describe a framework for developing probabilistic classifiers in natural language processing. Our focus is on formulating models that capture the most important interdependencies among features, to avoid overfitting the data while also characterizing the data well. The class of probability models and the associated inference techniques described here were developed in mathematical statistics, and are widely used in artificial intelligence and applied statistics. Our goal is to make this model selection framework accessible to researchers in NLP, and provide pointers to available software and important references. In addition, we describe how the quality of the three determinants of classifier performance (the features, the form of the model, and the parameter estimates) can be separately evaluated. We also demonstrate the classification performance of these models in a large-scale experiment involving the disambiguation of 34 words taken from the HECTOR word sense corpus (Hanks 1996). In 10-fold cross-validations, the model search procedure performs significantly better than naive Bayes on 6 of the words without being significantly worse on any of them.},
author = {Bruce, R.F. and Wiebe, J.},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Bruce, Wiebe - 1999 - Decomposable modeling in natural language processing.pdf:pdf},
issn = {08912017},
journal = {Computational Linguistics},
number = {2},
pages = {195--207},
publisher = {MIT Press},
title = {{Decomposable Modeling in Natural Language Processing}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Decomposable+Modeling+in+Natural+Language+Processing\#0},
volume = {25},
year = {1999}
}
@inproceedings{Pantel2002,
abstract = {Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses.},
address = {New York City, USA},
author = {Pantel, Patrick and Lin, Dekang},
booktitle = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
doi = {10.1145/775047.775138},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/PantelLin2002.pdf:pdf},
isbn = {1-58113-567-X},
keywords = {clustering,evaluation,machine learning,word sense discovery},
pages = {613--619},
publisher = {ACM Press},
title = {{Discovering word senses from text}},
url = {http://doi.acm.org/10.1145/775047.775138},
volume = {41},
year = {2002}
}
@book{Agirre2007b,
abstract = {Covers the topic of word sense disambiguation (WSD) including: the major algorithms, techniques, performance measures, results, philosophical issues, and applications. This book provides an overview of the research across the field. It teaches how to build and evaluate systems, and talks about what performance to expect.},
author = {Agirre, Eneko and Edmonds, Philip Glenny},
isbn = {402048084978},
publisher = {Springer-Verlag},
title = {{Word Sense Disambiguation: Algorithms and Applications}},
year = {2007}
}
@inproceedings{Lee2002,
abstract = {In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.},
address = {Philadelphia, PA},
author = {Lee, Yoong Keok and Ng, Hwee Tou},
booktitle = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing - EMNLP '02},
doi = {10.3115/1118693.1118699},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Lee2002.pdf:pdf},
pages = {41--48},
title = {{An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation}},
url = {http://portal.acm.org/citation.cfm?doid=1118693.1118699},
volume = {10},
year = {2002}
}
@inproceedings{Vasilescu2004,
address = {Lisbon, Portugal},
author = {Vasilescu, Florentina and Langlais, Philippe and Lapalme, Guy},
booktitle = {Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04)},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Vasilescu, Langlais, Lapalme - 2004 - Evaluating Variants of the Lesk Approach for Disambiguating Words.pdf:pdf},
pages = {633--636},
title = {{Evaluating Variants of the Lesk Approach for Disambiguating Words}},
year = {2004}
}
@inproceedings{Yarowsky1993,
abstract = {Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability a polysemous word has one sense per discourse. In this paper we show that for certain definitions of collocation, a polysemous word exhibits essentially only one sense per collocation. We test this empirical hypothesis for several definitions of sense and collocation, and discover that it holds with 90-99\% accuracy for binary ambiguities. We utilize this property in a disambiguation algorithm that achieves precision of 92\% using combined models of very local context.},
address = {Princeton, New Jersey},
author = {Yarowsky, David},
booktitle = {Proceedings of the workshop on Human Language Technology - HLT '93},
doi = {10.3115/1075671.1075731},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/OneSensePerCollocationYarowsky1993.pdf:pdf},
isbn = {1558603247},
pages = {266--271},
title = {{One Sense Per Collocation}},
year = {1993}
}
@inproceedings{Ide2004,
abstract = {The First Release of the American National Corpus (ANC) was made available in mid-fall, 2003. The data includes approximately 11 million words of American English, including written and spoken data and a variety of text types annotated for part of speech and lemma. The corpus is provided in XML format conformant to the XML Corpus Encoding Standard (XCES) (http://www.xml-ces.org), and is distributed in both a stand-off version (where annotation is in an XML document separate from the primary texts) and a merged version (where annotation is included in-line in the texts). The merged version includes annotation for part of speech and lemma produced by the Biber tagger; in stand-off annotation, in addition to the Biber tagging, morpho-syntactic annotations of the data are provided using the CLAWS 5 and 7 tagsets as well as several other tagsets.},
address = {Lisbon, Portugal},
author = {Ide, Nancy and Suderman, K},
booktitle = {Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04)},
pages = {1681--1684},
title = {{The American National Corpus First Release}},
year = {2004}
}
@misc{Landauer1998,
abstract = {Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.},
author = {Landauer, Thomas K and Foltz, Peter W. and Laham, Darrell},
booktitle = {Discourse Processes},
doi = {10.1080/01638539809545028},
isbn = {0163853X},
issn = {0163-853X},
number = {2-3},
pages = {259--284},
pmid = {14532333},
title = {{An introduction to latent semantic analysis}},
volume = {25},
year = {1998}
}



@article{Firth1957,
abstract = {Reprinted in: Palmer, F. R. (ed.) (1968). Selected Papers of J. R. Firth 1952-59, pages 168-205. Longmans, London. },
author = {Firth, J R},
journal = {Studies in Linguistic Analysis (special volume of the Philological Society)},
keywords = {classic linguistics meanign relatedness semantic},
pages = {1--32},
publisher = {The Philological Society},
title = {{A synopsis of linguistic theory 1930-55.}},
volume = {1952-59},
year = {1957}
}
@inproceedings{Agirre2007,
address = {Prague, Czech Republic},
author = {Agirre, Eneko and Soroa, Aitor},
booktitle = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
pages = {7--12},
series = {SemEval '07},
title = {{Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems}},
url = {http://dl.acm.org/citation.cfm?id=1621474.1621476},
year = {2007}
}
@article{Robinson2005,
abstract = {Designing usable geovisualization tools is an emerging problem in GIScience software development. We are often satisfied that a new method provides an innovative window on our data, but functionality alone is insufficient assurance that a tool is applicable to a problem in situ. As extensions of the static methods they evolved from, geovisualization tools are bound to enable new knowledge creation. We have yet to learn how to adapt techniques from interaction designers and usability experts toward our tools in order to maximize this ability. This is especially challenging because there is limited existing guidance for the design of usable geovisualization tools. Their design requires knowledge about the context of work within which they will be used, and should involve user input at all stages, as is the practice in any human-centered design effort. Toward that goal, we have employed a wide range of techniques in the design of ESTAT, an exploratory geovisualization toolkit for epidemiology. These techniques include; verbal protocol analysis, card-sorting, focus groups, and an in-depth case study. This paper reports the design process and evaluation results from our experience with the ESTAT toolkit.},
author = {Robinson, Anthony C and Chen, Jin and Lengerich, Eugene J and Meyer, Hans G and MacEachren, Alan M},
doi = {10.1559/152304005775194700},
issn = {15230406},
journal = {Cartography and Geographic Information Science},
number = {4},
pages = {243--255},
publisher = {NIH Public Access},
title = {{Combining usability techniques to design geovisualization tools for epidemiology}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2786201/},
volume = {32},
year = {2005}
}
@inproceedings{Purandare2004,
abstract = {This paper systematically compares unsuper- vised word sense discrimination techniques that cluster instances of a target word that oc- cur in raw text using both vector and similarity spaces. The context of each instance is repre- sented as a vector in a high dimensional fea- ture space. Discrimination is achieved by clus- tering these context vectors directly in vector space and also by Ô¨Ånding pairwise similarities among the vectors and then clustering in sim- ilarity space. We employ two different repre- sentations of the context in which a target word occurs. First order context vectors represent the context of each instance of a target word as a vector of features that occur in that con- text. Second order context vectors are an indi- rect representation of the context based on the average of vectors that represent the words that occur in the context. We evaluate the discrim- inated clusters by carrying out experiments us- ing sense‚Äìtagged instances of 24 SENSEVAL- 2 words and the well known Line, Hard and Serve sense‚Äìtagged corpora.},
address = {Boston, MA, USA},
author = {Purandare, Amruta and Pedersen, Ted},
booktitle = {Proceedings of the Conference on Computational Natural Language Learning},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/PurandarePedersen2004.pdf:pdf},
pages = {41--48},
title = {{Word Sense Discrimination by Clustering Contexts in Vector and Similarity Spaces}},
year = {2004}
}
@article{Wagner2006,
abstract = {Much of today‚Äôs organizational knowledge still exists outside of formal information repositories and often only in people‚Äôs heads. While organizations are eager to capture this knowledge, existing acquisition methods are not up to the task. Neither traditional artificial intelligencebased approaches nor more recent, less-structured knowledge management techniques have overcome the knowledge acquisition challenges. This article investigates knowledge acquisition bottlenecks and proposes the use of collaborative, conversational knowledge management to remove them. The article demonstrates the opportunity for more effective knowledge acquisition through the application of the principles of Bazaar style, open-source development. The article introduces wikis as software that enables this type of knowledge acquisition. It empirically analyzes the Wikipedia to produce evidence for the feasibility and effectiveness of the proposed approach.},
author = {Wagner, Christian},
doi = {10.4018/irmj.2006010104},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/KnowledgeAcquisitionBottleneckWagner2006.pdf:pdf},
isbn = {9781599049335},
issn = {1040-1628},
journal = {Information Resources Management Journal},
keywords = {knowledge acquisition,knowledge artifacts,knowledge management,open,source development,wiki},
number = {1},
pages = {70--83},
pmid = {1286},
title = {{Breaking the Knowledge Acquisition Bottleneck Through Conversational Knowledge Management}},
volume = {19},
year = {2006}
}
@inproceedings{Manning2014,
author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
file = {:Volumes/Bill/Documents/Uni/Watson-Projekt/Papers/StanfordCoreNlp2014.pdf:pdf},
pages = {55--60},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://www.aclweb.org/anthology/P/P14/P14-5010},
year = {2014}
}
@inproceedings{Baldwin1998,
abstract = {Scoring the performance of a system is an extremely important aspect of coreference algorithm performance. The score for a particular run is the single strongest measure of how well the system is performing and it can strongly determine directions for further improvements. In this paper, we present several different scoring algorithms and detail their respective strengths and weaknesses for varying classes of processing. We also demonstrate that tasks like information extraction have very different needs from information retrieval in terms of how to score the performance of coreference annotation. Introduction Scoring the performance of a system is an extremely important aspect of coreference algorithm performance. The score for a particular run is the single strongest measure of how well the system is performing and it can strongly determine directions for further improvements. In this paper, we present several different scoring algorithms and detail their respective strengths and w...},
address = {Pennsylvania, USA},
author = {Baldwin, Amit Bagga; Breck and Bagga, Amit and Baldwin, Breck},
booktitle = {First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference},
pages = {563--566},
title = {{Algorithms for scoring coreference chain}},
year = {1998}
}
@book{Bannore2009,
abstract = {In the last two decades, extensive literature 6-10 has been produced on the research of super-resolution image reconstruction. The research has witnessed the introduction of many techniques in order to achieve high-resolution images from a sequence of geometrically warped, blurred, noisy, and under-sampled low-resolution images. Each of these techniques is either an extension of some previous methodologies or differs from the other techniques in their assumptions of the observation model or the type of reconstruction method applied for achieving the high-resolution images. On a broader aspect, we divide the various super-resolution techniques depending upon the domain (spatial or frequency) they are based on.},
author = {Bannore, Vivek},
booktitle = {IterativeInterpolation SuperResolution Image Reconstruction},
doi = {10.1007/978-3-642-00385-1},
isbn = {9783642003844},
pages = {9 -- 17},
publisher = {Springer Berlin Heidelberg},
series = {Studies in Computational Intelligence},
title = {{Iterative-Interpolation Super-Resolution Image Reconstruction}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-00385-1},
volume = {195},
year = {2009}
}
@inproceedings{Dean2004,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
booktitle = {Proceedings of 6th Symposium on Operating Systems Design and Implementation},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
file = {:Users/jsimon/Pictures/MapReduceDean2004.pdf:pdf},
isbn = {9781595936868},
issn = {00010782},
pages = {137--149},
pmid = {11687618},
publisher = {San Francisco, CA},
title = {{MapReduce: Simplied Data Processing on Large Clusters}},
year = {2004}
}
@article{Agirre1995,
abstract = {This paper presents a method for the resolution of lexical ambiguity and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiment have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9510003},
author = {Agirre, Eneko and Rigau, German},
eprint = {9510003},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/9510003.pdf:pdf},
keywords = {conceptual distance,semcor,word sense disambiguation,wordnet},
pages = {7},
primaryClass = {cmp-lg},
title = {{A Proposal for Word Sense Disambiguation using Conceptual Distance}},
url = {http://arxiv.org/abs/cmp-lg/9510003},
year = {1995}
}
@article{Biemann2005,
author = {Biemann, Chris},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Chris\_Biemann.pdf:pdf},
number = {2},
journal = {LDV Forum},
pages = {75--93},
title = {{Ontology Learning from Text: A Survey of Methods}},
url = {http://wortschatz.uni-leipzig.de/~cbiemann/pub/2005/BiemannLDVOntology05.pdf},
volume = {20},
year = {2005}
}
@misc{Miller1995,
abstract = {This database links English nouns, verbs, adjectives, and adverbs to sets of synonims that are in turn link through semantic relations that determine word definitions.},
author = {Miller, George A.},
booktitle = {Communications of the ACM},
doi = {10.1145/219717.219748},
isbn = {1558602720},
issn = {00010782},
number = {11},
pages = {39--41},
pmid = {17081734},
title = {{WordNet: a lexical database for English}},
volume = {38},
year = {1995}
}


@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM}
}

@inproceedings{Lin2010,
abstract = {This half-day tutorial introduces participants to data-intensive text processing with the MapReduce programming model 1, using the open-source Hadoop implementation. The focus will be on scalability and the tradeoffs associated with distributed processing of large datasets. Content will include general discussions about algorithm design, presentation of illustrative algorithms, case studies in HLT applications, as well as practical advice in writing Hadoop programs and running Hadoop clusters.},
address = {Boulder, Colorado},
author = {Lin, Jimmy and Dyer, Chris},
booktitle = {Synthesis Lectures on Human Language Technologies},
doi = {10.2200/S00274ED1V01Y201006HLT007},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Dyer - 2010 - Data-Intensive Text Processing with MapReduce.pdf:pdf},
isbn = {1608453421},
issn = {1947-4040},
pages = {1--177},
title = {{Data-Intensive Text Processing with MapReduce}},
volume = {3},
year = {2010}
}
@article{Church1990,
abstract = {The term word association is used in a very particular sense in the$\backslash$npsycholinguistic literature. \{(Generally\} speaking, subjects respond$\backslash$nquicker than normal to the word nurse if it follows a highly associated$\backslash$nword such as doctor. ) We will extend the term to provide the basis$\backslash$nfor a statistical description of a variety of interesting linguistic$\backslash$nphenomena, ranging from semantic relations of the doctor/nurse type$\backslash$n(content word/content word) to lexico-syntactic co-occurrence constraints$\backslash$nbetween verbs and prepositions (content word/function word). This$\backslash$npaper will propose an objective measure based on the information$\backslash$ntheoretic notion of mutual information, for estimating word association$\backslash$nnorms from computer readable corpora. \{(The\} standard method of obtaining$\backslash$nword association norms, testing a few thousand subjects on a few$\backslash$nhundred words, is both costly and unreliable.) The proposed measure,$\backslash$nthe association ratio, estimates word association norms directly$\backslash$nfrom computer readable corpora, making it possible to estimate norms$\backslash$nfor tens of thousands of words.},
author = {Church, Kenneth Ward and Hanks, Patrick},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Church, Hanks - 1990 - Word association norms, mutual information, and lexicography.pdf:pdf},
issn = {08912017},
journal = {Computational Linguistics},
number = {1},
pages = {22--29},
title = {{Word association norms, mutual information, and lexicography}},
url = {http://portal.acm.org/citation.cfm?id=89095\&dl=},
volume = {16},
year = {1990}
}
@article{Leacock1998,
abstract = {Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck. We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora. We describe a statistical classifier that combines topical context with local cues to identify a word sense. The classifier is used to disambiguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples.},
author = {Leacock, Claudia and Miller, George A. and Chodorow, Martin},
doi = {10.3115/1075812.1075867},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/leacock.compling.24.1998.148.pdf:pdf},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {147--165},
title = {{Using Corpus Statistics and WordNet Relations for Sense Identification}},
url = {papers2://publication/uuid/F8B8B5B7-CEDF-44B0-A548-FD5849FDAB85},
volume = {24},
year = {1998}
}
@article{Chu-Carroll2012,
abstract = {A key phase in the DeepQA architecture is Hypothesis Generation, in which candidate system responses are generated for downstream scoring and ranking. In the IBM Watson‚Ñ¢ system, these hypotheses are potential answers to Jeopardy!‚Ñ¢ questions and are generated by two components: search and candidate generation. The search component retrieves content relevant to a given question from Watson's knowledge resources. The candidate generation component identifies potential answers to the question from the retrieved content. In this paper, we present strategies developed to use characteristics of Watson's different knowledge sources and to formulate effective search queries against those sources. We further discuss a suite of candidate generation strategies that use various kinds of metadata, such as document titles or anchor texts in hyperlinked documents. We demonstrate that a combination of these strategies brings the correct answer into the candidate answer pool for 87.17\% of all the questions in a blind test set, facilitating high end-to-end question-answering performance.},
author = {Chu-Carroll, J. and Fan, J. and Boguraev, B.K. and Carmel, D. and Sheinwald, D. and Welty, C.},
file = {:Volumes/Bill/Documents/Uni/Watson-Projekt/Papers/06 finding needles - search and candidate generation.pdf:pdf},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
keywords = {Computer architecture,Electronic publishing,Encyclopedias,Internet,Search engines,Search problems,Semantics,Syntactics},
number = {3.4},
pages = {6:1--6:12},
shorttitle = {IBM Journal of Research and Development},
title = {{Finding needles in the haystack: Search and candidate generation}},
volume = {56},
year = {2012}
}
@article{Hachey2013,
abstract = {Named Entity Linking (nel) grounds entity mentions to their corresponding node in a Knowledge Base (kb). Recently, a number of systems have been proposed for linking entity mentions in text to Wikipedia pages. Such systems typically search for candidate entities and then disambiguate them, returning either the best candidate or nil. However, comparison has focused on disambiguation accuracy, making it difficult to determine how search impacts performance. Furthermore, important approaches from the literature have not been systematically compared on standard data sets. We reimplement three seminal nel systems and present a detailed evaluation of search strategies. Our experiments find that coreference and acronym handling lead to substantial improvement, and search strategies account for much of the variation between systems. This is an interesting finding, because these aspects of the problem have often been neglected in the literature, which has focused largely on complex candidate ranking algorithms. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Hachey, Ben and Radford, Will and Nothman, Joel and Honnibal, Matthew and Curran, James R.},
doi = {10.1016/j.artint.2012.04.005},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/hachey-aij12-evaluating.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Disambiguation,Information extraction,Named Entity Linking,Semi-structured resources,Wikipedia},
pages = {130--150},
title = {{Evaluating entity linking with wikipedia}},
volume = {194},
year = {2013}
}
@article{Rosenberg2007,
abstract = {We present V-measure, an external entropy- based cluster evaluation measure. V- measure provides an elegant solution to many problems that affect previously de- fined cluster evaluation measures includ- ing 1) dependence on clustering algorithm or data set, 2) the problem of matching, where the clustering of only a portion of data points are evaluated and 3) accurate evalu- ation and combination of two desirable as- pects of clustering, homogeneity and com- pleteness. We compare V-measure to a num- ber of popular cluster evaluation measures and demonstrate that it satisfies several de- sirable properties of clustering solutions, us- ing simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.},
author = {Rosenberg, Andrew and Hirschberg, Julia},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/v\_measure-emnlp07.pdf:pdf},
journal = {Computational Linguistics},
pages = {410--420},
title = {{V-measure: A conditional entropy-based external cluster evaluation measure}},
url = {http://acl.ldc.upenn.edu/D/D07/D07-1043.pdf},
volume = {1},
year = {2007}
}
@article{Maron2010,
abstract = {Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie ...},
author = {Maron, Yariv and Lamar, Michael and Bienenstock, Elie},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/SCodeMarov2010.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing Systems 23},
pages = {1567--1575},
title = {{Sphere Embedding: An Application to Part-of-Speech Induction}},
year = {2010}
}
@book{Ebert,
author = {Ebert, David S.},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Lam, Munzner - 2010 - A Guide to Visual Multi-Level Interface Design From Synthesis of Empirical Study Evidence.pdf:pdf},
isbn = {9781608455928},
keywords = {user interaction},
mendeley-tags = {user interaction},
title = {{A Guide to Visual Multi-Level Interface Design From Synthesis of Empirical Study Evidence}}
}
@article{Buitelaar2004,
abstract = {The volume presents current research in ontology learning, addressing three per- spectives: methodologies that have been proposed to automatically extract information from texts and to give a structured organization to such knowledge, including approaches based on machine learning techniques; evaluation methods for ontology learning, aiming at defining procedures and metrics for a quantitative evaluation of the ontology learning task; and finally application scenarios that make ontology learning a challenging area in the context of real applications such as bio-informatics.},
author = {Buitelaar, Paul and Cimiano, Philipp and Magnini, Bernardo},
doi = {10.1.1.70.3041},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Buitelaar2013.pdf:pdf},
isbn = {978-1-58603-523-5},
journal = {Learning},
keywords = {knowledge acquisition,ontology learning,text mining},
pages = {1--10},
title = {{Ontology Learning from Text : An Overview}},
year = {2004}
}
@book{Dodge2008,
abstract = {Geographic Visualization: Concepts, Tools and Applications is a state-of-the-art review of the latest developments in the subject. It examines how new concepts, methods and tools can be creatively applied to solve problems relevant to a wide range of topics. The text covers the impact of three-dimensional displays on user interaction along with the potentialities in animation and clearly explains how to create temporally sensitive visualizations. It also explores the potential for handling mobile data and representing uncertainty; as well as the role of participatory visualization systems and exploratory methods. Hallmark Features: An introduction to the diverse forms of geographic visualization which draws upon a number of theoretical perspectives and disciplines to provide an insightful commentary on new methods, techniques and tools. Richly illustrated in full colour throughout, including numerous relevant case studies and accessible discussions of important visualization concepts to enable clearer understanding for non-technical audiences. Chapters are written by leading scholars and researchers in a range of cognate fields, including, cartography, GIScience, architecture, art, urban planning and computer graphics with case studies drawn from Europe, North America and Australia This book is an invaluable resource for all graduate students, researchers and professionals working in the geographic information sector, computer graphics and cartography.},
author = {Dodge, Martin and McDerby, Mary and Turner, Martin},
isbn = {0470515112},
keywords = {map-based visualization},
mendeley-tags = {map-based visualization},
pages = {325},
publisher = {Wiley},
title = {{Geographic Visualization: Concepts, Tools and Applications}},
url = {http://books.google.com/books?id=71rRm1xDEYwC\&printsec=frontcover\&dq=Geographic+Visualization:+Concepts,+Tools+and+Applications\&hl=en\&ei=etDPToadHY26hAfokpCiDQ\&sa=X\&oi=book\_result\&ct=result\&resnum=1\&ved=0CDMQ6AEwAA\#v=onepage\&q\&f=false},
year = {2008}
}
@inproceedings{Jurgens2013,
address = {Atlanta, Georgia, USA},
author = {Jurgens, David and Klapaftis, Ioannis},
booktitle = {Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Jurgens2013-SemEval2013Task13.pdf:pdf},
pages = {290--299},
title = {{Semeval-2013 task 13: Word sense induction for graded and non-graded senses}},
url = {http://www.aclweb.org/anthology/S/S13/S13-2049.pdf},
volume = {2},
year = {2013}
}
@article{Lam2010,
author = {Lam, H and Munzner, T},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Lam, Munzner - 2010 - A Guide to Visual Multi-Level Interface Design From Synthesis of Empirical Study Evidence.pdf:pdf},
journal = {Synthesis Lectures on Visualization},
number = {1},
pages = {1--117},
publisher = {Morgan \& Claypool Publishers},
title = {{A Guide to Visual Multi-Level Interface Design From Synthesis of Empirical Study Evidence}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00308ED1V01Y201011VIS001},
volume = {1},
year = {2010}
}
@inproceedings{Nivre2006,
abstract = {We introduce MaltParser, a data-driven parser generator for dependency parsing. Given a treebank in dependency format, MaltParser can be used to induce a parser for the language of the treebank. MaltParser supports several parsing algorithms and learning algorithms, and allows user-defined feature models, consisting of arbitrary combinations of lexical features, part-of-speech features and dependency features. MaltParser is freely available for research and educational purposes and has been evaluated empirically on Swedish, English, Czech, Danish and Bulgarian.},
address = {Genoa, Italy},
author = {Nivre, J and Hall, J and Nilsson, J},
booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/maltparserNive2006.pdf:pdf},
pages = {2216--2219},
publisher = {European Language Resources Association (ELRA)},
title = {{MaltParser: A Data-Driven Parser-Generator for Dependency Parsing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.64.5584\&amp;rep=rep1\&amp;type=pdf},
volume = {6},
year = {2006}
}
@inproceedings{Klein2002,
address = {Philadelphia, PA},
author = {Klein, Dan and Toutanova, Kristina and Ilhan, H. Tolga and Kamvar, Sepandar D. and Manning, Christopher D.},
booktitle = {Proceedings of the ACL-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions},
doi = {10.3115/1118675.1118686},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Klein2002.pdf:pdf},
pages = {74--80},
title = {{Combining Heterogeneous Classifiers for Word-Sense Disambiguation}},
url = {http://portal.acm.org/citation.cfm?doid=1118675.1118686},
volume = {8},
year = {2002}
}
@inproceedings{Pradhan2007a,
abstract = {This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ‚Äì Lexical Sample, Semantic Role Labeling (SRL) and All-Words respectively. We tab- ulate and analyze the results of participating systems.},
address = {Prague, Czech Republic},
author = {Pradhan, S and Loper, E and Dligach, D and Palmer, M},
booktitle = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Pradhan2007.pdf:pdf},
pages = {87--92},
title = {{SemEval-2007 Task 17: English Lexical Sample, SRL and All Words}},
url = {papers2://publication/uuid/74521E4E-811F-4EB7-A1BE-973D69EBC6C2},
year = {2007}
}
@inproceedings{Suchanek2007,
address = {Banff, Alberta, Canada},
author = {Suchanek, Fabian M and Kasneci, Gjergji and Weikum, Gerhard},
booktitle = {Proceedings of the 16th International Conference on World Wide Web},
doi = {10.1145/1242572.1242667},
isbn = {978-1-59593-654-7},
keywords = {WordNet,wikipedia},
pages = {697--706},
publisher = {ACM},
series = {WWW '07},
title = {{Yago: A Core of Semantic Knowledge}},
url = {http://doi.acm.org/10.1145/1242572.1242667},
year = {2007}
}
@inproceedings{Jurgens2011,
address = {Edinburgh, Scotland},
author = {Jurgens, David and Stevens, K},
booktitle = {Proceedings of the First Workshop on Unsupervised Learning in NLP},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Jurgens, Stevens - 2011 - Measuring the impact of sense similarity on word sense induction.pdf:pdf},
pages = {113--123},
title = {{Measuring the Impact of Sense Similarity on Word Sense Induction}},
url = {http://dl.acm.org/citation.cfm?id=2140472},
year = {2011}
}
@misc{UCRProject,
author = {FBI},
title = {{Uniform Crime Report Project, http://www.fbi.gov/about-us/cjis/ucr/ucr}},
url = {http://www.fbi.gov/about-us/cjis/ucr/ucr}
}
@article{Wang2012,
abstract = {Detecting semantic relations in text is an active problem area in natural-language processing and information retrieval. For question answering, there are many advantages of detecting relations in the question text because it allows background relational knowledge to be used to generate potential answers or find additional evidence to score supporting passages. This paper presents two approaches to broad-domain relation extraction and scoring in the DeepQA question-answering framework, i.e., one based on manual pattern specification and the other relying on statistical methods for pattern elicitation, which uses a novel transfer learning technique, i.e., relation topics. These two approaches are complementary; the rule-based approach is more precise and is used by several DeepQA components, but it requires manual effort, which allows for coverage on only a small targeted set of relations (approximately 30). Statistical approaches, on the other hand, automatically learn how to extract semantic relations from the training data and can be applied to detect a large amount of relations (approximately 7,000). Although the precision of the statistical relation detectors is not as high as that of the rule-based approach, their overall impact on the system through passage scoring is statistically significant because of their broad coverage of knowledge. View full abstract},
author = {Wang, C and Kalyanpur, A and Boguraev, B K},
doi = {10.1147/JRD.2012.2187239},
file = {:Volumes/Bill/Documents/Uni/Watson-Projekt/Papers/09 relation extraction and scoring.pdf:pdf},
issn = {00188646},
journal = {IBM Journal of},
number = {3},
pages = {1--12},
title = {{Relation extraction and scoring in DeepQA}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=\&arnumber=6177734\&contentType=Journals+\&+Magazines\&sortType=asc\_p\_Sequence\&filter=AND(p\_IS\_Number:6177717)},
volume = {56},
year = {2012}
}
@techreport{Zhao2001,
abstract = {In recent years, we have witnessed a tremendous growth in the volume of text documents available on the Internet, digital libraries, news sources, and company-wide intranets. This has led to an increased interest in developing methods that can help users to effectively navigate, summarize, and organize this information with the ultimate goal of helping them to find what they are looking for. Fast and high-quality document clustering algorithms play an important role towards this goal as they have been shown to provide both an intuitive navigation/browsing mechanism by organizing large amounts of information into a small number of meaningful clusters as well as to greatly improve the retrieval performance either via cluster-driven dimensionality reduction, term-weighting, or query expansion. This ever-increasing importance of document clustering and the expanded range of its applications led to the development of a number of new and novel algorithms with different complexity-quality trade-offs. Among them, a class of clustering algorithms that have relatively low computational requirements are those that treat the clustering problem as an optimization process which seeks to maximize or minimize a particular clustering criterion function defined over the entire clustering solution. The focus of this paper is to evaluate the performance of different criterion functions for the problem of clustering documents. Our study involves a total of eight different criterion functions, three of which are introduced in this paper and five that have been proposed in the past. Our evaluation consists of both a comprehensive experimental evaluation involving fifteen different datasets, as well as an analysis of the characteristics of the various criterion functions and their effect on the clusters they produce. Our experimental results show that there are a set of criterion functions that consistently outperform the rest, and that some of the newly proposed criterion function lead to the best overall results. Our theoretical analysis of the criterion function shows that their relative performance depends on (i) the degree to which they can correctly operate when the clusters are of different tightness, and (ii) the degree to which they can lead to reasonably balanced clusters.},
address = {Minneapolis},
author = {Zhao, Ying and Karypis, George},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/ZhaoKarypis2001.pdf:pdf},
institution = {Department of Computer Science and Engineering, University of Minnesota},
isbn = {1581126204},
title = {{Criterion Functions for Document Clustering: Experiments and Analysis}},
year = {2001}
}
@article{Navigli2009,
abstract = {Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.},
author = {Navigli, Roberto},
doi = {10.1145/1459352.1459355},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Navigli2009.pdf:pdf},
isbn = {0360-0300},
issn = {0360-0300},
journal = {ACM Computing Surveys (CSUR)},
number = {2},
pages = {10},
publisher = {ACM},
title = {{Word sense disambiguation: A survey}},
url = {http://portal.acm.org/citation.cfm?id=1459355},
volume = {41},
year = {2009}
}
@techreport{Zhao2002,
abstract = {Fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters, and in greatly improving the retrieval performance either via cluster-driven dimensionality reduction, term-weighting, or query expansion. This ever-increasing importance of document clustering and the expanded range of its applications led to the development of a number of novel algorithms and new clustering criterion functions, especially in the context of partitional clustering. The focus of this paper is to experimentally evaluate the performance of seven different global criterion functions in the context of agglomerative clustering algorithms and compare the clustering results of agglomerative algorithms and partitional algorithms for each one of the criterion functions. Our experimental evaluation shows that for every criterion function, partitional algorithms always lead to better clustering results than agglomerative algorithms, which suggests that partitional clustering algorithms are well-suited for clustering large document datasets due to not only their relatively low computational requirements, but also comparable or even better clustering performance.},
address = {Minneapolis},
author = {Zhao, Ying and Karypis, George},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Zhao2002.pdf:pdf},
institution = {Department of Computer Science and Engineering, University of Minnesota},
title = {{Comparison of Agglomerative and Partitional Document Clustering Algorithms}},
year = {2002}
}
@book{Maciejewski,
author = {Maciejewski, Ross},
booktitle = {Statistics},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Maciejewski - Unknown - Data Representations, Transformations, and Statistics for Visual Reasoning.pdf:pdf},
isbn = {9781608456253},
title = {{Data Representations, Transformations, and Statistics for Visual Reasoning}}
}
@article{Denkowski2009,
abstract = {... to be clustered and the features considered are defined in the graph space rather ... One shortcoming of many clustering approaches using feature vectors involves the overlooking of very ... For the case of word  sense  induction , each vertex represents a word and each hyperedge ...},
author = {Denkowski, Michael},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Denkowski - 2009 - A Survey of Techniques for Unsupervised Word Sense Induction.pdf:pdf},
journal = {Language \& Statistics II Literature Review},
pages = {1--18},
title = {{A Survey of Techniques for Unsupervised Word Sense Induction}},
url = {https://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/mdenkows/pdf/wsi2009.pdf},
year = {2009}
}
@article{Harris1954,
abstract = {Harris maintains that it is possible to define a linguistic structure solely in terms of the "distributions" (= patterns of co-occurrences) of its elements. There is no parallel meaning-structure which can aid in describing formal structure. Meaning is partly a function of distribution.},
author = {Harris, Zellig S.},
journal = {Word},
number = {23},
pages = {146--162},
title = {{Distributional structure.}},
volume = {10},
year = {1954}
}
@article{Keim2008,
author = {Keim, Daniel A and Mansmann, Florian and Thomas, Jim and Ziegler, Hartmut},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Keim et al. - 2008 - Visual Analytics Scope and Challenges.pdf:pdf},
journal = {Information Visualization},
pages = {76--90},
title = {{Visual Analytics : Scope and Challenges}},
year = {2008}
}
@article{Yuret2012,
abstract = {Lexical substitutes have found use in areas such as paraphrasing, text simplification, machine translation, word sense disambiguation, and part of speech induction. However the computational complexity of accurately identifying the most likely substitutes for a word has made large scale experiments difficult. In this letter we introduce a new search algorithm, FASTSUBS, that is guaranteed to find the K most likely lexical substitutes for a given word in a sentence based on an n-gram language model. The computation is sublinear in both K and the vocabulary size V. An implementation of the algorithm and a dataset with the top 100 substitutes of each token in the WSJ section of the Penn Treebank are available at http://goo.gl/jzKH0.},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.5407v2},
author = {Yuret, Deniz},
doi = {10.1109/LSP.2012.2215587},
eprint = {arXiv:1205.5407v2},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Lexical substitutes,statistical language modeling},
number = {11},
pages = {725--728},
title = {{FASTSUBS: An efficient and exact procedure for finding the most likely lexical substitutes based on an n-gram language model}},
volume = {19},
year = {2012}
}
@article{Lenat1995,
address = {New York City, USA},
author = {Lenat, Douglas B},
doi = {10.1145/219717.219745},
issn = {0001-0782},
journal = {Communications of the ACM},
month = nov,
number = {11},
pages = {33--38},
publisher = {ACM},
title = {{CYC: A Large-scale Investment in Knowledge Infrastructure}},
url = {http://doi.acm.org/10.1145/219717.219745},
volume = {38},
year = {1995}
}
@article{Niu2007,
author = {Niu, Zheng-yu and Niu, Zheng-yu and Ji, Dong-hong and Ji, Dong-hong and Tan, Chew-lim and Tan, Chew-lim},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/I2R.pdf:pdf},
journal = {Computational Linguistics},
pages = {177--182},
title = {{I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense Disambiguation, and English Word Sense Disambiguation}},
year = {2007}
}
@phdthesis{Biemann2007,
abstract = {‚Ä¢ In supervised systems, the data as presented to a machine learning algorithm is fully labelled. That means: all examples are presented with a classiÔ¨Åcation that the machine is meant to reproduce. For this, a classiÔ¨Åer is learned from the data, the process of assigning labels to yet unseen instances is called classiÔ¨Åcation.$\backslash$n‚Ä¢ In semi-supervised systems, the machine is allowed to additionally take unlabelled data into account. Due to a larger data basis, semi-supervised systems often outperform their supervised counterparts using the same labelled examples (see (Zhu, 2005) for a survey on semi-supervised methods and (Sarkar and Haffari, 2006) for a summary regarding NLP and semisupervision). The reason for this improvement is that more unlabelled data enables the systemtomodel the inherent structure of the data more accurately.$\backslash$n‚Ä¢ Bootstrapping, also called self-training, is a form of learning that is designed to use even less training examples, thereforesometimes called weakly-supervised. Bootstrapping (see (Biemann, 2006a) for an introduction) starts with a few training examples, trains a classiÔ¨Åer, and uses thought-to-be positive examples as yielded by this classiÔ¨Åer for retraining. As the set of training examples grows, the classiÔ¨Åer improves, provided that not too many negative examples are misclassiÔ¨Åed as positive, which could lead to deterioration of performance.$\backslash$n‚Ä¢ Unsupervised systems are not provided any training examples at all and conduct clustering. This is the division of data instances into several groups, (see (Manning and Sch\"{u}tze, 1999, Ch. 14) and (Berkhin, 2002) for an overview of clustering methods in NLP). The results of clustering algorithms are data driven, hencemore ‚Äônatural‚Äô and better suited to the underlying structure of the data. This advantage is also itsmajor drawback: without a possibility to tell the machine what to do (like in classiÔ¨Åcation), it is difÔ¨Åcult to judge the quality of clustering results in a conclusive way. But the absence of training example preparation makes the unsupervised paradigm very appealing.},
author = {Biemann, Christian},
doi = {10.1007/978-3-642-25923-4\_8},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Biemann - 2007 - Unsupervised and Knowledge-free Natural Language Processing in the Structure Discovery Paradigm.pdf:pdf},
school = {University of Leipzig},
title = {{Unsupervised and Knowledge-free Natural Language Processing in the Structure Discovery Paradigm}},
year = {2007}
}

@INPROCEEDINGS{Goldhahn2012b,
  author = {Goldhahn, Dirk and Eckart, Thomas and Quasthoff, Uwe},
  title = {{Building Large Monolingual Dictionaries at the Leipzig Corpora Collection:
	From 100 to 200 Languages}},
  booktitle = {Proceedings of the Eight International Conference on Language Resources
	and Evaluation (LREC'12)},
  year = {2012},
  pages = {759--765},
  address = {Istanbul, Turkey},
  isbn = {978-2-9517408-7-7}
}

@book{Gigaword2011,
author = {Robert Parker and David Graff and Junbo Kong and Ke Chen and Kazuaki Maeda},
title = {English Gigaword Fifth Edition},
publisher = {Linguistic Data Consortium}, 
address = {Philadelphia, USA},
year = {2011}
}




@inproceedings{Suarez2002,
address = {Taipei, Taiwan},
author = {Su\'{a}rez, Armando and Palomar, Manuel},
booktitle = {Proceedings of the 19th international conference on Computational linguistics},
doi = {http://dx.doi.org/10.3115/1072228.1072343},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Su\'{a}rez, Palomar - 2002 - A maximum entropy-based word sense disambiguation system.pdf:pdf},
pages = {1--7},
title = {{A maximum entropy-based word sense disambiguation system}},
year = {2002}
}
@article{Trendalyst,
author = {Sheng, Tan Yong and Wei, Derrick Low Xuan and Kiat, Tay Wei},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Sheng, Wei, Kiat - 2011 - TRENDALYST A Geo- visual Analytics Application for Trending.pdf:pdf},
journal = {Innovation},
pages = {1--11},
title = {{TRENDALYST : A Geo- visual Analytics Application for Trending}},
year = {2011}
}
@inproceedings{Lau2013,
address = {Atlanta, Georgia, USA},
author = {Lau, Jey Han and Cook, Paul and Baldwin, Timothy},
booktitle = {Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/unimelbLau2013.pdf:pdf},
pages = {307--311},
title = {{unimelb: Topic Modelling-based Word Sense Induction}},
url = {http://www.aclweb.org/anthology/S13-2051},
volume = {2},
year = {2013}
}
@book{Fellbaum1998,
abstract = {WordNet, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets. The purpose of this volume is twofold. First, it discusses the design of WordNet and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains.},
author = {Fellbaum, Christiane},
booktitle = {British Journal Of Hospital Medicine London England 2005},
chapter = {Combining},
editor = {Fellbaum, Christiane},
issn = {17508460},
pages = {423},
pmid = {21561289},
publisher = {MIT Press},
series = {Language, Speech, and Communication},
title = {{WordNet: An Electronic Lexical Database}},
url = {http://acl.ldc.upenn.edu/J/J99/J99-2008.pdf},
volume = {71},
year = {1998}
}
@article{Dunning1993,
abstract = {Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.},
author = {Dunning, Ted},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Dunning1993.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
pages = {61--74},
title = {{Accurate Methods for the Statistics of Surprise and Coincidence}},
url = {http://portal.acm.org/citation.cfm?id=972454},
volume = {19},
year = {1993}
}
@article{TruliaCrimeMaps,
author = {{Trulia Inc.}},
title = {{Trulia Crime Maps, http://www.trulia.com/crime/}},
url = {http://www.trulia.com/crime/}
}
@phdthesis{Mallery1988,
author = {Mallery, John C.},
school = {MIT, Cambridge, MA},
title = {{Thinking About Foreign Policy: Finding an Appropriate Role for Artificially Intelligent Computers}},
year = {1988}
}
@phdthesis{VanDongen2000,
abstract = {UU Home page. Title: Graph clustering by flow simulation. Author: Dongen, Stijn Marinus van. Year: 2000. Document type: Dissertation. Abstract: show / hideDit proefschrift heeft als onderwerp het clusteren},
author = {van Dongen, Stjin},
booktitle = {Graph stimulation by flow clustering},
doi = {10.1016/j.cosrev.2007.05.001},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/vanDongen2000.pdf:pdf},
isbn = {90-393-2408-5},
issn = {15740137},
school = {University of Utrecht},
title = {{Graph clustering}},
volume = {PhD thesis},
year = {2000}
}
@inproceedings{Kilgarriff2000,
abstract = {There are now many computer programs for automatically determining which sense a word is being used in. One would like to be able to say which were better, which worse, and also which words, or varieties of language, presented particular problems to which programs. In 1998 a first evaluation exercise, SENSEVAL, took place. The English component of the exercise is described, and results presented.},
address = {Athens, Grece},
author = {Kilgarriff, Adam and Kilgarriff, Adam and Rosenzweig, Joseph and Rosenzweig, Joseph},
booktitle = {In Proceedings of the 2nd International Conference on Language Resources and Evaluation},
doi = {10.1.1.23.9860},
title = {{English Senseval: Report and Results}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.9860},
year = {2000}
}
@article{Jarvelin2002,
abstract = {Modern large retrieval environments tend to overwhelm their users by their large output. Since all documents are not of equal relevance to their users, highly relevant documents should be identified and ranked first for presentation. In order to develop IR techniques in this direction, it is necessary to develop evaluation approaches and methods that credit IR methods for their ability to retrieve highly relevant documents. This can be done by extending traditional evaluation methods, that is, recall and precision based on binary relevance judgments, to graded relevance judgments. Alternatively, novel measures based on graded relevance judgments may be developed. This article proposes several novel measures that compute the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. The first one accumulates the relevance scores of retrieved documents along the ranked result list. The second one is similar but applies a discount factor to the relevance scores in order to devaluate late-retrieved documents. The third one computes the relative-to-the-ideal performance of IR techniques, based on the cumulative gain they are able to yield. These novel measures are defined and discussed and their use is demonstrated in a case study using TREC data: sample system run results for 20 queries in TREC-7. As a relevance base we used novel graded relevance judgments on a four-point scale. The test results indicate that the proposed measures credit IR methods for their ability to retrieve highly relevant documents and allow testing of statistical significance of effectiveness differences. The graphs based on the measures also provide insight into the performance IR techniques and allow interpretation, for example, from the user point of view.},
author = {J\"{a}rvelin, Kalervo and Kek\"{a}l\"{a}inen, Jaana},
doi = {10.1145/582415.582418},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Jarvelin2002.pdf:pdf},
isbn = {1046-8188},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
number = {4},
pages = {422--446},
title = {{Cumulated Gain-based Evaluation of IR Techniques}},
volume = {20},
year = {2002}
}
@article{WhereDoYouGo,
author = {Lehrburger, Steven},
title = {{Where Do You Go, http://www.wheredoyougo.net/}},
url = {http://www.wheredoyougo.net/},
year = {2010}
}
@incollection{Leacock1998b,
abstract = {Word sense identification is the mapping between words in a text and their appropriate senses in a lexicon. l Some level of word sense identification is required for virtually all natural-language-processing applications. Text must be disambiguated before it can be...},
author = {Leacock, Claudia and Chodorow, Martin},
booktitle = {WordNet: An electronic lexical database.},
chapter = {13},
editor = {Fellbaum, Christiane},
pages = {265--283},
publisher = {MIT Press},
title = {{Combining Local Context and WordNet Similarity for Word Sense Identification}},
year = {1998}
}
@misc{Spotcrime,
author = {{ReportSee Inc.}},
title = {{Spotcrime Project, https://www.spotcrime.com/}},
url = {https://www.spotcrime.com/}
}
@article{Schutze1998,
abstract = {This paper presents context-group discrimination, a disambiguation algorithm based on cluster- ing. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.},
archivePrefix = {arXiv},
arxivId = {schuetze1998automatic},
author = {Sch\"{u}tze, Hinrich},
eprint = {schuetze1998automatic},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Schuetze1998.pdf:pdf},
issn = {08912017},
journal = {Computational Linguistics},
number = {1},
pages = {97--123},
title = {{Automatic Word Sense Discrimination}},
volume = {24},
year = {1998}
}
@article{Unger2012,
abstract = {As an increasing amount of RDF data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the RDF data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a SPARQL template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches.},
author = {Unger, Christina and B\"{u}hmann, Lorenz},
journal = {Proceedings of the 21st international conference on World Wide Web},
keywords = {SPARQL,natural language patterns,question answering,semantic web},
pages = {639--648},
title = {{Template-based question answering over RDF data}},
url = {http://dl.acm.org/citation.cfm?doid=2187836.2187923$\backslash$nhttp://dl.acm.org/citation.cfm?id=2187923},
year = {2012}
}
@techreport{Wee2010,
author = {Wee, Heng Low},
file = {:Volumes/Bill/No-Backup/Downloads/low\_wee\_urop.pdf:pdf},
institution = {Department of Computer Science, National University of Singapore},
title = {{Word Sense Prediction Using Decision Trees}},
year = {2010}
}
@article{Sheridan2001,
abstract = {In recent years there has been great interest in the evidence that with experience human decision-making becomes "automatic", "recognition-primed", "naturalistic", "situated", and "ecological". Advocates of these approaches have rejected classical decision theory as a description of decision-making in real tasks. This rejection poses a challenge to better understand human automaticity by extending normative decision modeling. Two approaches, both based on state-space reduction, are discussed. There is also the challenge of relating these models to what is being learned in neurobiology, and again a model is presented which potentially explains how behavior can become automatic. Implications for computer-based automation of decision aiding and control are also proposed, including a revised four-stage format for scaling levels of automation.},
author = {Sheridan, Thomas B},
doi = {10.1016/S1367-5788(01)00009-8},
issn = {13675788},
journal = {Annual Reviews in Control},
pages = {89--97},
title = {{Rumination on automation, 1998}},
url = {http://www.sciencedirect.com/science/article/B6V0H-43N7F8X-8/2/a0bd285448683d3e309cfa7b5fa60cc5},
volume = {25},
year = {2001}
}
@incollection{Fagan2011,
address = {Boca Raton, US},
author = {Gen\c{c}ay, Ramazan and Fagan, Steven},
booktitle = {Handbook of Empirical Economics and Finance},
editor = {Ullah, Aman and Gill, David},
isbn = {978-1-4200-7036},
pages = {133--154},
publisher = {Taylor \& Francis},
title = {{An Introduction to Textual Econometrics}},
year = {2011}
}
@phdthesis{Curran2003,
abstract = {Institute for Communicating and Collaborative Systems},
author = {Curran, James Richard},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Curran2003.pdf:pdf},
keywords = {Natural Language Processing,thesauri},
school = {University of Edinburgh},
title = {{From Distributional to Semantic Similarity}},
url = {http://hdl.handle.net/1842/563},
year = {2003}
}
@book{Nielsen1993,
abstract = {"The purpose of Jakob Nielsen's Usability Engineering is to help nontechnical people improve the systems so thatthey are not only error-free but also easier and more pleasant to use, and more efficient. It is a book that ...shows ushow to change the world and does so admirably....One of this book's strengths is that it provides a wide selection ofmethods for improving systems, and allows for the unavoidable constraints of the real world." -NEW SCIENTIST Written by the author of the best-selling HyperText \& HyperMedia, this book is an excellent guide to the methodsof usability engineering. The book provides the tools needed to avoid usability surprises and improve product quality.Step-by-step information on which method to use at various stages during the development lifecycle are included,along with detailed information on how to run a usability test and the unique issues relating to international usability. KEY FEATURESEmphasizes cost-effective methods that developers can implement immediately.Instructs readers about which methods to use and when to use them throughout the development lifecycle, ultimately helping in cost-benefit analysis. Shows readers how to avoid the four most frequently listed reasons for delay in software projects. Includes detailed information on how to run a usability test. Covers unique issues of international usability. Features an extensive bibliography allowing readers to find additional information. Written by an internationally renowned expert in the field and the author of the best-selling HyperText \& HyperMedia.},
author = {Nielsen, Jakob},
booktitle = {Usability Engineering},
chapter = {6},
editor = {Nielsen, J},
isbn = {0125184069},
issn = {10772626},
number = {3},
pages = {362},
pmid = {18369261},
publisher = {Morgan Kaufmann},
title = {{Usability Engineering}},
url = {http://www.useit.com/jakob/useengbook.html},
volume = {44},
year = {1993}
}
@article{Lin2001,
abstract = {Many applications dealing with textual information require classification of words into semantic classes (or concepts). However, manually constructing semantic classes is a tedious task. In this paper, we present an algorithm, UNICON, for UNsupervised Induction of CONcepts. Some advantages of UNICON over previous approaches include the ability to classify words with low frequency counts, the ability to cluster a large number of elements in a high-dimensional space, and the ability to classify previously unknown words into existing clusters. Furthermore, since the algorithm is unsupervised, a set of concepts may be constructed for any corpus.},
author = {Lin, Dekang and Lin, Dekang and Pantel, Patrick and Pantel, Patrick},
doi = {10.1145/502512.502558},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Lin2001.pdf:pdf},
isbn = {158113391X},
journal = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '01},
pages = {317--322},
title = {{Induction of semantic classes from natural language text}},
url = {http://portal.acm.org/citation.cfm?doid=502512.502558},
year = {2001}
}
@inproceedings{Gale1992,
abstract = {It is well-known that there are polysemous words like sentence whose "meaning" or "sense" depends on the context of use. We have recently reported on two new word-sense disambiguation systems, one trained on bilin- gual material (the Canadian Hansards) and the other trained on monolingual material (Roget's Thesaurus and Grolier's Encyclopedia). As this work was nearing com- pletion, we observed a very strong discourse effect. That is, if a polysemous word such as sentence appears two or more times in a well-written discourse, it is extremely likely that they will all share the same sense. This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong (98\%). This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algo- rithm. In addition, it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint.},
address = {Harriman, New York},
author = {Gale, Wa and Church, Kw and Yarowsky, David},
booktitle = {Proceedings of the Workshop on Speech and Natural Language},
doi = {10.3115/1075527.1075579},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/OneSensePerDiscourseGale1992.pdf:pdf},
isbn = {1558602720},
pages = {233--237},
title = {{One sense per discourse}},
url = {http://dl.acm.org/citation.cfm?id=1075579},
year = {1992}
}
@inproceedings{Agirre:2006:TGA:1610075.1610157,
address = {Stroudsburg, PA, USA},
author = {Agirre, Eneko and Mart\'{\i}nez, David and de Lacalle, Oier L\'{o}pez and Soroa, Aitor},
booktitle = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing},
isbn = {1-932432-73-6},
pages = {585--593},
series = {EMNLP '06},
title = {{Two Graph-based Algorithms for State-of-the-art WSD}},
url = {http://dl.acm.org/citation.cfm?id=1610075.1610157},
year = {2006}
}
@misc{Safecast,
title = {{Safecast, http://blog.safecast.org}},
url = {http://blog.safecast.org}
}
@article{Teh2006,
abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the ‚ÄúChinese restaurant franchise.‚Äù We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.6738v2},
author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
doi = {10.1198/016214506000000302},
eprint = {arXiv:1210.6738v2},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/HierarchicalDirichletProcessTeh2006.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {476},
pages = {1566--1581},
pmid = {242869700023},
title = {{Hierarchical Dirichlet Processes}},
volume = {101},
year = {2006}
}
@article{Prudhommeaux2008,
abstract = {RDF is a directed, labeled graph data format for representing information in the Web. This specification defines the syntax and semantics of the SPARQL query language for RDF. SPARQL can be used to express queries across diverse data sources, whether the data is stored natively as RDF or viewed as RDF via middleware. SPARQL contains capabilities for querying required and optional graph patterns along with their conjunctions and disjunctions. SPARQL also supports extensible value testing and constraining queries by source RDF graph. The results of SPARQL queries can be results sets or RDF graphs.},
author = {Prud'hommeaux, Eric and Seaborne, Andy},
editor = {Prud'hommeaux, Eric and Seaborne, Andy},
institution = {W3C},
journal = {W3C Recommendation},
pages = {1--106},
pmid = {31},
publisher = {W3C},
series = {Working Draft},
title = {{SPARQL Query Language for RDF}},
url = {http://www.w3.org/TR/rdf-sparql-query/},
volume = {2009},
year = {2008}
}
@book{Manning1999,
abstract = {The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis. New to the Second Edition Greater prominence of statistical approaches New applications section Broader multilingual scope to include Asian and European languages, along with English An actively maintained wiki that provides online resources, supplementary information, and up-to-date developments Divided into three sections, the book first surveys classical techniques, including both symbolic and empirical approaches. The second section focuses on statistical approaches in natural language processing. In the final section of the book, each chapter describes a particular class of application, from Chinese machine translation to information visualization to ontology construction to biomedical text mining. Fully updated with the latest developments in the field, this comprehensive, modern handbook emphasizes how to implement practical language processing tools in computational systems.},
author = {Manning, Christopher D and Sch\"{u}tze, Hinrich},
isbn = {1420085921},
pages = {678},
title = {{Foundations of Natural Language Processing}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Foundations+of+Natural+Language+Processing\#3},
year = {1999}
}
@article{Keim,
author = {Keim, Daniel A and Bak, Peter and Bertini, Enrico and Oelke, Daniela and Spretke, David},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Keim et al. - Unknown - Advanced Visual Analytics Interfaces.pdf:pdf},
isbn = {9781450300766},
journal = {Information Visualization},
keywords = {data mining,growth ring maps,information visualization,visual analytics},
mendeley-tags = {growth ring maps},
title = {{Advanced Visual Analytics Interfaces}}
}
@inproceedings{Biemann2012,
address = {Istanbul, Turkey},
author = {Biemann, Chris},
booktitle = {Proceedings of the 8th International Conference on Language Resources and Evaluation},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Biemann\_TWSI\_LREC2012.pdf:pdf},
isbn = {978-2-9517408-7-7},
keywords = {crowdsourcing,lexical substitution,word sense inventory},
pages = {4038--4042},
title = {{Turk Bootstrap Word Sense Inventory 2.0: A Large-Scale Resource for Lexical Substitution}},
year = {2012}
}
@article{Towell1998,
abstract = {A word sense disambiguator that is able to distinguish among the many senses of common words that are found in general-purpose, broad-coverage lexicons would be useful. For example, experiments have shown that, given accurate sense disambiguation. the lexical relations encoded in lexicons such as WordNet can be exploited to improve the effectiveness of information retrieval systems. This paper describes a classifier whose accuracy may be sufficient for such a purpose, The classifier combines the output of a neural network that learns topical context with the output of a network that learns local context to distinguish among the senses of highly ambiguous words. The accuracy of the classifier is tested on three words, the noun line, the verb serve, and the adjective hard; the classifier has an average accuracy of 87\%, 90\%, and 81\%, respectively, when forced to choose a sense for all test cases. When the classifier is not forced to choose a sense and is trained on a subset of the available senses, it rejects test cases containing unknown senses as well as rest cases it would misclassify if forced to select a sense. Finally when there are few labeled training examples available, we describe an extension of our training method that uses information extracted from unlabeled examples to improve classification accuracy.},
author = {Towell, George and Voorhees, Ellen},
file = {:Volumes/Bill/No-Backup/Downloads/J98-1005.pdf:pdf},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {125--145},
title = {{Disambiguating highly ambiguous words}},
url = {papers2://publication/uuid/50BA625C-0E51-423E-9E21-8CB0895BBD11},
volume = {24},
year = {1998}
}
@article{Agirre2007a,
abstract = {This paper describes a graph-based unsu- pervised system for induction and clas- sification. The system performs a two stage graph based clustering where a co- occurrence graph is first clustered to com- pute similarities against contexts. The con- text similarity matrix is pruned and the re- sulting associated graph is clustered again by means of a random-walk type algorithm. The system relies on a set of parameters that have been tuned to fit the corpus data. The system has participated in tasks 2 and 13 of the SemEval-2007 competition, on word sense induction and Web people search, re- spectively, with mixed results.},
author = {Agirre, Eneko and Agirre, Eneko and Soroa, Aitor and Soroa, Aitor},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/UBC-AS-Agirre2007.pdf:pdf},
journal = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
pages = {346--349},
title = {{UBC-AS: A Graph Based Unsupervised System for Induction and Classification}},
year = {2007}
}
@inproceedings{Bollacker2008,
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
address = {Vancouver, Canada},
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
booktitle = {SIGMOD 08 Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
doi = {10.1145/1376616.1376746},
isbn = {9781605581026},
issn = {07308078},
pages = {1247--1250},
title = {{Freebase: a collaboratively created graph database for structuring human knowledge}},
url = {http://doi.acm.org/10.1145/1376616.1376746},
year = {2008}
}
@misc{Crimemapping,
title = {{Crimemapping Project, http://www.crimemapping.com}},
url = {http://www.crimemapping.com}
}
@article{Nasiruddin2013,
abstract = {Word Sense Disambiguation (WSD), the process of automatically identifying the meaning of a polysemous word in a sentence, is a fundamental task in Natural Language Processing (NLP). Progress in this approach to WSD opens up many promising developments in the field of NLP and its applications. Indeed, improvement over current performance levels could allow us to take a first step towards natural language understanding. Due to the lack of lexical resources it is sometimes difficult to perform WSD for under-resourced languages. This paper is an investigation on how to initiate research in WSD for under-resourced languages by applying Word Sense Induction (WSI) and suggests some interesting topics to focus on.},
author = {Nasiruddin, M},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Nasiruddin - 2013 - A State of the Art of Word Sense Induction A Way Towards Word Sense Disambiguation for Under-Resourced Languages.pdf:pdf},
journal = {arXiv preprint arXiv:1310.1425},
pages = {17--21},
title = {{A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages}},
url = {http://arxiv.org/abs/1310.1425},
volume = {2},
year = {2013}
}
@article{Velldal2005,
author = {Velldal, E},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Velldal05.pdf:pdf},
journal = {Proceedings of the 7th International conference on  \ldots},
pages = {1--13},
title = {{A fuzzy clustering approach to word sense discrimination}},
url = {http://heim.ifi.uio.no/~erikve/pubs/Velldal05.pdf},
year = {2005}
}
@article{Robinson2009,
author = {Robinson, Anthony C},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Robinson - 2009 - VISUAL HIGHLIGHTING METHODS FOR GEOVISUALIZATION.pdf:pdf},
journal = {Elsevier},
number = {1987},
title = {{VISUAL HIGHLIGHTING METHODS FOR GEOVISUALIZATION}},
url = {http://www.geovista.psu.edu/publications/2009/ACR\_ICA\_2009.pdf},
year = {2009}
}
@article{Basile2014,
author = {Basile, Pierpaolo and Caputo, Annalina and Semeraro, Giovanni},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/C14-1151.pdf:pdf},
journal = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
pages = {1591--1600},
title = {{An Enhanced Lesk Word Sense Disambiguation Algorithm through a Distributional Semantic Model}},
year = {2014}
}
@inproceedings{Riedl2014,
address = {Dublin, Ireland},
author = {Riedl, Martin and Alles, Irina and Biemann, Chris},
booktitle = {Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)},
pages = {1435--1446},
title = {{Combining Supervised and Unsupervised Parsing for Distributional Similarity}},
year = {2014}
}
@article{Veronis2004,
abstract = {This article describes an algorithm called HyperLex that is capable of automatically determining word uses in a textbase without recourse to a dictionary. The algorithm makes use of the specific properties of word cooccurrence graphs, which are shown as having "small world" properties. Unlike earlier dictionary-free methods based on word vectors, it can isolate highly infrequent uses (as rare as 1\% of all occurrences) by detecting "hubs" and high-density components in the cooccurrence graphs. The algorithm is applied here to information retrieval on the Web, using a set of highly ambiguous test words. An evaluation of the algorithm showed that it only omitted a very small number of relevant uses. In addition, HyperLex offers automatic tagging of word uses in context with excellent precision (97\%, compared to 73\% for baseline tagging, with an 82\% recall rate). Remarkably good precision (96\%) was also achieved on a selection of the 25 most relevant pages for each use (including highly infrequent ones). Finally, HyperLex is combined with a graphic display technique that allows the user to navigate visually through the lexicon and explore the various domains detected for each word use. ¬© 2004 Elsevier Ltd. All rights reserved.},
author = {V\'{e}ronis, Jean},
doi = {10.1016/j.csl.2004.05.002},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/HyperLex.pdf:pdf},
isbn = {3344295349},
issn = {08852308},
journal = {Computer Speech and Language},
keywords = {graphic interface,graphs,information retrieval,small worlds,word sense disambiguation},
pages = {223--252},
title = {{HyperLex: Lexical cartography for information retrieval}},
volume = {18},
year = {2004}
}
@inproceedings{Leacock1993,
abstract = {The three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polyse- mous word by using knowledge about patterns of word co- occurrences. The techniques were based on Bayesian decision theory, neural networks, and content vectors as used in in- formation retrieval. To understand these methods better, we posed s very specific problem: given a set of contexts, each containing the noun line in a known sense, construct a classi- fier that selects the correct sense of line for new contexts. To see how the degree of polysemy affects performance, results from three- and slx-sense tasks are compared. The results demonstrate that each of the techniques is able to distinguish six senses of line with an accuracy greater than 70\%. Furthermore, the response patterns of the classifiers are, for the most part, statistically indistinguishable from one another. Comparison of the two tasks suggests that the degree of difficulty involved in resolving individual senses is a greater performance factor than the degree of polysemy.},
address = {Princeton, New Jersey},
author = {Leacock, Claudia and Towell, Geoffrey and Voorhees, Ellen},
booktitle = {Proceedings of HLT '93 Proceedings of the workshop on Human Language Technology},
doi = {10.3115/1075671.1075730},
file = {:Volumes/Bill/No-Backup/Downloads/H93-1051.pdf:pdf},
isbn = {1-55860-324-7},
pages = {260--265},
publisher = {Association for Computational Linguistics},
title = {{Corpus-based Statistical Sense Resolution}},
year = {1993}
}
@article{Hovy2013,
abstract = {Recent years have seen a great deal of work that exploits collaborative, semi-structured content for Artificial Intelligence (AI) and Natural Language Processing (NLP). This special issue of the Artificial Intelligence Journal presents a variety of state-of-the-art contributions, each of which illustrates the substantial impact that work on leveraging semi-structured content is having on AI and NLP as it continuously fosters new directions of cutting-edge research. We contextualize the papers collected in this special issue by providing a detailed overview of previous work on collaborative, semi-structured resources. The survey is made up of two main logical parts: in the first part, we present the main characteristics of collaborative resources that make them attractive for AI and NLP research; in the second part, we present an overview of how these features have been exploited to tackle a variety of long-standing issues in the two fields, in particular the acquisition of large amounts of machine-readable knowledge, and its application to a wide range of tasks. The overall picture shows that not only are semi-structured resources enabling a renaissance of knowledge-rich AI techniques, but also that significant advances in high-end applications that require deep understanding capabilities can be achieved by synergistically exploiting large amounts of machine-readable structured knowledge in combination with sound statistical AI and NLP techniques. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Hovy, Eduard and Navigli, Roberto and Ponzetto, Simone Paolo},
doi = {10.1016/j.artint.2012.10.002},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/HovyNavigliPonzetto2013.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Knowledge acquisition,Knowledge-rich methods,Semantic networks},
pages = {2--27},
publisher = {Elsevier Science Publishers Ltd.},
title = {{Collaboratively built semi-structured content and Artificial Intelligence: The story so far}},
url = {http://dx.doi.org/10.1016/j.artint.2012.10.002},
volume = {194},
year = {2013}
}


@incollection{banerjee2002adapted,
  title={An adapted Lesk algorithm for word sense disambiguation using WordNet},
  author={Banerjee, Satanjeev and Pedersen, Ted},
  booktitle={Computational linguistics and intelligent text processing},
  pages={136--145},
  year={2002},
  publisher={Springer}
}

@inproceedings{mihalcea2004pagerank,
  title={PageRank on semantic networks, with application to word sense disambiguation},
  author={Mihalcea, Rada and Tarau, Paul and Figa, Elizabeth},
  booktitle={Proceedings of the 20th international conference on Computational Linguistics},
  pages={1126},
  year={2004},
  organization={Association for Computational Linguistics}
}


@inproceedings{Lesk1986,
abstract = {An abstract is not available.},
address = {Toronto, Ontario, Canada},
author = {Lesk, Michael},
booktitle = {Proceedings of the 5th annual international conference on Systems documentation},
doi = {10.1145/318723.318728},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Lesk - 1986 - Automatic Sense Disambiguation Using Machine Readable Dictionaries How to Tell a Pine Cone from an Ice Cream Cone.pdf:pdf},
isbn = {0-89791-224-1},
keywords = {automatic,dictionaries,disambiguation,machine,readable,sense,using},
pages = {24--26},
publisher = {ACM},
title = {{Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone}},
url = {http://dx.doi.org/10.1145/318723.318728},
year = {1986}
}
@article{Amigo2009-ALT,
abstract = {There is a wide set of evaluation metrics available to compare the quality of text clustering algorithms. In this article, we define a few intuitive formal constraints on such metrics which shed light on which aspects of the quality of a clustering are captured by different metric families. These formal constraints are validated in an experiment involving human assessments, and compared with other constraints proposed in the literature. Our analysis of a wide range of metrics shows that only BCubed satisfies all formal constraints. We also extend the analysis to the problem of overlapping clustering, where items can simultaneously belong to more than one cluster. As Bcubed cannot be directly applied to this task, we propose a modified version of Bcubed that avoids the problems found with other metrics.},
author = {Amig\'{o}, Enrique and Gonzalo, Julio and Artiles, Javier and Verdejo, Felisa},
doi = {10.1007/s10791-008-9066-8},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Amigo2009.pdf:pdf},
isbn = {1386-4564 (Print) 1573-7659 (Online)},
issn = {13864564},
journal = {Information Retrieval},
keywords = {Clustering,Evaluation metrics,Formal constraints},
number = {4},
pages = {461--486},
title = {{A comparison of extrinsic clustering evaluation metrics based on formal constraints}},
volume = {12},
year = {2009}
}
@inproceedings{Pedersen2000a,
abstract = {This paper presents a corpus-based approach to word sense disambiguation that$\backslash$nbuilds an ensemble of Naive Bayesian classifiers, each of which is based on$\backslash$nlexical features that represent co--occurring words in varying sized windows of$\backslash$ncontext. Despite the simplicity of this approach, empirical results$\backslash$ndisambiguating the widely studied nouns line and interest show that such an$\backslash$nensemble achieves accuracy rivaling the best previously published results.},
address = {Seattle, Washington},
archivePrefix = {arXiv},
arxivId = {cs/0005006v1},
author = {Pedersen, Ted},
booktitle = {Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference (NAACL 2000)},
eprint = {0005006v1},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/NaiveBayesWSDPedersen.pdf:pdf},
keywords = {Computation and Language},
pages = {63--69},
primaryClass = {cs},
title = {{A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation}},
url = {http://arxiv.org/abs/cs/0005006v1},
year = {2000}
}
@article{Gale1992a,
author = {Gale, William A and Church, Kenneth W and Yarowsky, David and Laboratories, Bell and J, Murray Hill N},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Gale et al. - 1992 - Work on Statistical Methods for Word Sense Disambiguation.pdf:pdf},
journal = {AAAI Fall Symposium on Probabilistic Approaches to Natural Language},
pages = {54--60},
title = {{Work on Statistical Methods for Word Sense Disambiguation}},
year = {1992}
}
@inproceedings{Sanderson1994,
address = {Dublin, Ireland},
author = {Sanderson, Mark},
booktitle = {Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval},
keywords = {IR},
publisher = {Springer-Verlag},
title = {{Word sense disambiguation and information retrieval}},
url = {http://portal.acm.org/citation.cfm?id=188548\&dl=},
year = {1994}
}
@misc{OaklandCrimespotting,
title = {{Oakland Crimespotting Project, http://oakland.crimespotting.org}},
url = {http://oakland.crimespotting.org}
}
@inproceedings{Bordag2006,
abstract = {In this paper a novel solution to automatic and unsupervised word sense induction (WSI) is introduced. It represents an instantiation of the ‚Äòone sense per collocation‚Äô observation (Gale et al., 1992). Like most existing approaches it utilizes clustering of word co-occurrences. This approach differs from other approaches to WSI in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs. The combination with a two-step clustering process using sentence co-occurrences as features allows for accurate results. Additionally, a novel and likewise automatic and unsupervised evaluation method inspired by Schutze‚Äôs (1992) idea of evalu- ¬® ation of word sense disambiguation algorithms is employed. Offering advantages like reproducability and independency of a given biased gold standard it also enables automatic parameter optimization of the WSI algorithm},
address = {Trento, Italy},
author = {Bordag, Stefan},
booktitle = {11th Conference of the European Chapter of the ACL},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/BordagEACL06.pdf:pdf},
isbn = {1-932432-59-0},
pages = {137--144},
title = {{Word Sense Induction : Triplet-Based Clustering and Automatic Evaluation}},
year = {2006}
}
@inproceedings{Rekimoto2007,
abstract = {Continuous logging of a person's geographical position is required for various "life-log" applications, such as memory aids, automatic blog generation, and life pattern analysis. GPS is one way of logging, but it is unable to track movements indoors, and hence cannot track peoplefs ordinary activities. We propose a WiFi-based location detection technology for location logging. It detects a device's location from received WiFi beacon signals. It works indoors and outdoors, andits estimated accuracy is often comparable to that of GPS. We built WiFi-based location logging systems based on a smart phone and a keychain-like device using custom hardware. These prototypes record WiFi information every few minutes, and this information is converted into actual location logs. We describe some life patterns created by analyzing these location logs. We also discuss various application examples and ideas for when continuous location logging becomes commonplace.},
author = {Rekimoto, J and Miyaki, T and Ishizawa, Takaaki},
booktitle = {Locationand contextawareness third international symposium LoCA 2007 Oberpfaffenhofen Germany September 2021 2007 proceedings},
doi = {10.1007/978-3-540-75160-1\_3},
editor = {Hightower, Jeffrey and Schiele, Bernt and Strang, Thomas},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Rekimoto, Miyaki, Ishizawa - Unknown - LifeTag WiFi-based Continuous Location Logging for Life Pattern Analysis.pdf:pdf},
isbn = {3540751599},
keywords = {map-based visualization},
mendeley-tags = {map-based visualization},
pages = {35},
publisher = {Springer-Verlag New York Inc},
series = {Lecture Notes in Computer Science},
title = {{LifeTag: WiFi-based continuous location logging for life pattern analysis}},
url = {http://portal.acm.org/citation.cfm?id=1777235.1777239},
volume = {4718},
year = {2007}
}
@article{Atkins1993,
author = {Atkins, S.},
journal = {Acta Linguistica Hun- garica},
pages = {5--72},
title = {{Tools for computer-aided corpus lexicography: The Hector project.}},
volume = {41},
year = {1993}
}
@article{Daelemans1998,
abstract = {We show that in language learning, contrary to received wisdom, keeping exceptional training instances in memory can be beneficial for generalization accuracy. We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks: grapheme-to-phoneme conversion, part-of-speech tagging, prepositional-phrase attachment, and base noun phrase chunking. In a first series of experiments we combine memory-based learning with training set editing techniques, in which instances are edited based on their typicality and class prediction strength. Results show that editing exceptional instances (with low typicality or low class prediction strength) tends to harm generalization accuracy. In a second series of experiments we compare memory-based learning and decision-tree learning methods on the same selection of tasks, and find that decision-tree learning often performs worse than memory-based learning. Moreover, the decrease in performance can be linked to the degree of abstraction from exceptions (i.e., pruning or eagerness). We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms.},
archivePrefix = {arXiv},
arxivId = {cs/9812021},
author = {Daelemans, Walter and Bosch, Antal Van Den and Zavrel, Jakub},
doi = {10.1.1.49.7747},
eprint = {9812021},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Daelemans1999.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {decision-tree,edited nearest neighbor classifier,memory-based learning,natural language learning},
pages = {31},
primaryClass = {cs},
title = {{Forgetting Exceptions is Harmful in Language Learning}},
url = {http://arxiv.org/abs/cs/9812021},
volume = {41},
year = {1998}
}
@article{W,
author = {W, W},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/05c5.pdf:pdf},
title = {{Evaluation of Wordnet-based Similarity 5.1}}
}
@article{TruthAboutCrime,
author = {BBC},
title = {{The Truth About Crime, http://www.bbc.co.uk/truthaboutcrime/crimemap/}},
url = {http://www.bbc.co.uk/truthaboutcrime/crimemap/}
}
@article{Pedersen2007,
author = {Pedersen, Ted},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/umnd2-sval4.pdf:pdf},
journal = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
pages = {394--397},
title = {{UMND2 : SenseClusters Applied to the Sense Induction Task of SENSEVAL-4}},
year = {2007}
}
@inproceedings{Hope2013,
address = {Samos, Greece},
author = {Hope, David and Keller, Bill},
booktitle = {Proceedings of the 14th International Conference on Computational Linguistics and Intelligent Text Processing - Volume Part I},
doi = {10.1007/978-3-642-37247-6\_30},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/MaxMaxHope2013.pdf:pdf},
isbn = {9783642372469},
issn = {03029743},
pages = {368--381},
publisher = {Springer-Verlag},
title = {{MaxMax: A Graph-based Soft Clustering Algorithm Applied to Word Sense Induction}},
year = {2013}
}
@article{Rao2013,
abstract = {In the menagerie of tasks for information extraction, entity linking is a new beast that has drawn a lot of attention from NLP practi- tioners and researchers recently. Entity Linking, also referred to as record linkage or entity resolution, involves aligning a textual mention of a named-entity to an appropriate entry in a knowledge base, which may or may not contain the entity. This has manifold applications ranging from linking patient health records to maintaining personal credit files, pre- vention of identity crimes, and supporting law enforcement. We discuss the key challenges present in this task and we present a high-performing system that links entities using max-margin ranking.We also summarize recent work in this area and describe several open research problems.},
author = {Rao, Delip and McNamee, Paul and Dredze, Mark},
doi = {10.1007/978-3-642-28569-1\_5},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/EntityLinkingRao2013.pdf:pdf},
isbn = {978-3-642-28568-4},
journal = {Multi-source, Multilingual Information Extraction and Summarization},
keywords = {entity disambiguation,entity linking,entity resolution,knowledge base population,named entities,record linkage},
pages = {93--115},
title = {{Entity Linking : Finding Extracted Entities in a Knowledge Base}},
url = {http://www.cs.jhu.edu/~delip/entity\_linking.pdf$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-642-28569-1\_5},
year = {2013}
}
@article{Litkowski2007,
abstract = {The SemEval-2007 task to disambiguate prepositions was designed as a lexical sample task. A set of over 25,000 instances was developed, covering 34 of the most frequent English prepositions, with two-thirds of the instances for training and one-third as the test set. Each instance identified a preposition to be tagged in a full sentence taken from the FrameNet corpus (mostly from the British National Corpus). Definitions from the Oxford Dictionary of English formed the sense inventories. Three teams participated, with all achieving supervised results significantly better than baselines, with a high fine-grained precision of 0.693. This level is somewhat similar to results on lexical sample tasks with open class words, indicating that significant progress has been made. The data generated in the task provides ample opportunitites for further investigations of preposition behavior.},
author = {Litkowski, Ken and Hargraves, Orin},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Litkowski, Hargraves - 2007 - \{SenEval\}-2007 Task 06 Word-Sense Disambiguation of Preposition.pdf:pdf},
journal = {Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
keywords = {compLx,preposition},
pages = {24--29},
title = {{\{SenEval\}-2007 Task 06: Word-Sense Disambiguation of Preposition}},
year = {2007}
}
@article{GrowthRingMaps,
abstract = {Spatiotemporal analysis of sensor logs is a challenging research field due to three facts: a) traditional two-dimensional maps do not support multiple events to occur at the same spatial location, b) three-dimensional solutions introduce ambiguity and are hard to navigate, and c) map distortions to solve the overlap problem are unfamiliar to most users. This paper introduces a novel approach to represent spatial data changing over time by plotting a number of non-overlapping pixels, close to the sensor positions in a map. Thereby, we encode the amount of time that a subject spent at a particular sensor to the number of plotted pixels. Color is used in a twofold manner; while distinct colors distinguish between sensor nodes in different regions, the colors' intensity is used as an indicator to the temporal property of the subjects' activity. The resulting visualization technique, called Growth Ring Maps, enables users to find similarities and extract patterns of interest in spatiotemporal data by using humans' perceptual abilities. We demonstrate the newly introduced technique on a dataset that shows the behavior of healthy and Alzheimer transgenic, male and female mice. We motivate the new technique by showing that the temporal analysis based on hierarchical clustering and the spatial analysis based on transition matrices only reveal limited results. Results and findings are cross-validated using multidimensional scaling. While the focus of this paper is to apply our visualization for monitoring animal behavior, the technique is also applicable for analyzing data, such as packet tracing, geographic monitoring of sales development, or mobile phone capacity planning.},
author = {Bak, Peter and Mansmann, Florian and Janetzko, Halldor and Keim, Daniel a},
doi = {10.1109/TVCG.2009.182},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Bak et al. - Unknown - Spatiotemporal analysis of sensor logs using growth ring maps.pdf:pdf},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Alzheimer Disease,Animal,Animal: physiology,Animals,Behavior,Cluster Analysis,Computational Biology,Computational Biology: methods,Computer Graphics,Disease Models,Female,Genetically Modified,Male,Mice,Spatial Behavior,Spatial Behavior: physiology,Time Factors,growth ring maps},
mendeley-tags = {growth ring maps},
number = {6},
pages = {913--20},
pmid = {19834154},
title = {{Spatiotemporal analysis of sensor logs using growth ring maps.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19834154},
volume = {15}
}
@inproceedings{Hutchison2012,
abstract = {We tackle the problem of improving the relevance of automatically selected tags in large-scale ontology-based information systems. Contrary to traditional settings where tags can be chosen arbitrarily, we focus on the problem of recommending tags (e.g., concepts) directly from a collaborative, user-driven ontology. We compare the effectiveness of a series of approaches to select the best tags ranging from traditional IR techniques such as TF/IDF weighting to novel techniques based on ontological distances and latent Dirichlet allocation. All our experiments are run against a real corpus of tags and documents extracted from the ScienceWise portal, which is connected to ArXiv.org and is currently used by growing number of researchers. The datasets for the experiments are made available online for reproducibility purposes.},
author = {Hutchison, David and Mitchell, John C},
booktitle = {11th International Semantic Web Conference},
doi = {10.1007/978-3-642-35176-1},
editor = {Cudr\'{e}-Mauroux, Philippe and Heflin, Jeff and Sirin, Evren and Tudorache, Tania and Euzenat, J\'{e}r\^{o}me and Hauswirth, Manfred and Parreira, Josiane Xavier and Hendler, Jim and Schreiber, Guus and Bernstein, Abraham and Blomqvist, Eva},
isbn = {9783642351754},
pages = {704},
publisher = {Springer},
title = {{The semantic web - ISWC 2012}},
year = {2012}
}
@inproceedings{Yarowsky1995,
abstract = {This paper presents an unsupervised learn- ing algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints - that words tend to have one sense per discourse and one sense per collocation - exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96\%.},
address = {Cambridge, Massachusetts},
author = {Yarowsky, David},
booktitle = {Proceedings of the 33rd annual meeting on Association for Computational Linguistics},
doi = {10.3115/981658.981684},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Yarowsky.pdf:pdf},
isbn = {0736-587X},
issn = {0736587X},
pages = {189--196},
title = {{Unsupervised Word Sense Disambiguation Rivaling Supervised Methods}},
year = {1995}
}
@article{Andrienko2008,
author = {Andrienko, Gennady and Andrienko, Natalia and Keim, Daniel and Maceachren, Alan M},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Andrienko et al. - 2008 - Challenging Problems of Geospatial Visual Analytics.pdf:pdf},
title = {{Challenging Problems of Geospatial Visual Analytics}},
year = {2008}
}
@inproceedings{Ng1996,
abstract = {In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named \{$\backslash$sc Lexas\}, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed. \{$\backslash$sc Lexas\} achieves a higher accuracy on the common data set, and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of \{$\backslash$sc WordNet\}.},
address = {Santa Cruz, CA},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9606032},
author = {Ng, Hwee Tou and Lee, Hian Beng},
booktitle = {Proceedings of the 34th Meeting of the Association for Computational Linguistics (ACL-96)},
doi = {10.1.1.14.4304},
eprint = {9606032},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/9606032.pdf:pdf},
pages = {40--47},
primaryClass = {cmp-lg},
publisher = {Association for Computational Linguistics},
title = {{Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach}},
url = {http://arxiv.org/abs/cmp-lg/9606032},
year = {1996}
}
@inproceedings{Ng1997a,
abstract = {Recent advances in large-scale, broad coverage part-of-speech tagging and syntactic parsing have been achieved in no small part due to the availability of large amounts of online, human-annotated corpora. In this paper, I argue that a large, human sensetagged corpus is also critical as well as necessary to achieve broad coverage, high accuracy word sense disambiguation, where the sense distinction is at the level of a good desk-top dictionary such as WORD- NET. Using the sense-tagged corpus of 192,800 word occurrences reported in (Ng and Lee, 1996), I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classio tier. I also estimate the amount of hu- man sense-tagged corpus and the manual annotation effort needed to build a largescale, broad coverage word sense disambiguation program which can significantly outperform the most-frequent-sense classifier.},
address = {Washington, D.C.},
author = {Ng, Hwee Tou},
booktitle = {Tagging Text with Lexical Semantics: Why, What, and How? Workshop},
doi = {10.1.1.11.4199},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Ng2007.pdf:pdf},
title = {{Getting serious about word sense disambiguation}},
url = {http://acl.ldc.upenn.edu/W/W97/W97-0201.pdf},
year = {1997}
}
@article{Agirre1996,
abstract = {This paper presents a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluated against SemCor, the sense-tagged version of the Brown Corpus.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9606007},
author = {Agirre, Eneko and Rigau, German},
doi = {10.3115/992628.992635},
eprint = {9606007},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Agirre, Rigau - 1996 - Word Sense Disambiguation using Conceptual Density.pdf:pdf},
pages = {8},
primaryClass = {cmp-lg},
title = {{Word Sense Disambiguation using Conceptual Density}},
url = {http://arxiv.org/abs/cmp-lg/9606007},
year = {1996}
}
@inproceedings{Gliozzo2013,
address = {Seatlle, Washington},
author = {Gliozzo, Alfio and Biemann, Chris and Riedl, Martin and Coppola, Bonaventura and Glass, Michael R and Hatem, Matthew and Heights, Yorktown},
booktitle = {Proceedings of TextGraphs-8 Graph-based Methods for Natural Language Processing},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Gliozzo et al. - 2013 - JoBimText Visualizer A Graph-based Approach to Contextualizing Distributional Similarity.pdf:pdf},
isbn = {9781937284978},
pages = {6--10},
title = {{JoBimText Visualizer : A Graph-based Approach to Contextualizing Distributional Similarity}},
volume = {1},
year = {2013}
}
@phdthesis{Dorow2007,
abstract = {MOTIVATION: The lexicon develops with its users. New words appear and old ones adopt new meanings to accommodate new technology, recent scientific discoveries or current affairs. Some parts of the lexicon grow incredibly fast. In particular those pertaining to booming fields such as the biosciences or computer technology. The lively research activity in these areas produces a flow of publications teeming with new terminology. The sheer volume of data and information makes computer-based tools for text processing ever more essential. Most of these systems rely heavily on knowledge about words and their meanings as contained in a lexicon. General lexicons are often not adequate for describing a given domain. They contain many rare words and word senses which are completely unrelated to the domain in question. At the same time, they fail to cover relevant words. Since it has become impracticable to maintain lexicons manually, automatic techniques for classifying words are needed. In this thesis we take a graph-theoretic approach to the automatic acquisition of word meanings. We represent the nouns in a text in form of a semantic graph consisting of words (the nodes) and relationships between them (the links). Links in the graph are based on cooccurrence of nouns in lists. We find that valuable information about the meaning of words and their interactions can be extracted from the resulting semantic structure. In the following we briefly describe the different semantic phenomena explored in this thesis. The structure of the thesis very much follows the structure of this introduction.},
author = {Dorow, Beate},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Dorow - 2007 - A graph model for words and their meanings.pdf:pdf},
issn = {0385-5414},
pages = {187},
school = {University of Stuttgart},
title = {{A Graph Model for Words and their Meanings}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.94.1001\&amp;rep=rep1\&amp;type=pdf},
year = {2007}
}
@article{Navigli2012,
abstract = {We present an automatic approach to the construction of BabelNet, a very large, wide-coverage multilingual semantic network. Key to our approach is the integration of lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition, Machine Translation is applied to enrich the resource with lexical information for all languages. We first conduct in vitro experiments on new and existing gold-standard datasets to show the high quality and coverage of BabelNet. We then show that our lexical resource can be used successfully to perform both monolingual and cross-lingual Word Sense Disambiguation: thanks to its wide lexical coverage and novel semantic relations, we are able to achieve state-of the-art results on three different SemEval evaluation tasks. ?? 2012 Elsevier B.V.},
author = {Navigli, Roberto and Ponzetto, Simone Paolo},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/AIJ\_2012\_Navigli\_Ponzetto.pdf:pdf},
journal = {Artificial Intelligence},
keywords = {Graph algorithms,Knowledge acquisition,Semantic networks,Word sense disambiguation},
pages = {217--250},
title = {{BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network}},
volume = {193},
year = {2012}
}
@article{Katz2006,
author = {Katz, Phil and Goldsmith-Pinkham, P},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Katz2006.pdf:pdf},
title = {{Word sense disambiguation using latent semantic analysis}},
url = {http://www.sccs.swarthmore.edu/users/07/pkatz1/cs65f06-final.pdf},
year = {2006}
}
@book{Mazza2009,
author = {Mazza, Riccardo},
booktitle = {October},
doi = {10.1007/978-1-84800-219-7},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Mazza - 2009 - Introduction to Information Visualization.pdf:pdf},
isbn = {9781848002180},
keywords = {map-based visualization},
mendeley-tags = {map-based visualization},
number = {978-1-84800-218-0},
pages = {139},
publisher = {Springer-Verlag New York Inc},
title = {{Introduction to Information Visualization}},
url = {http://www.springerlink.com/index/10.1007/978-1-84800-219-7},
volume = {1},
year = {2009}
}
@article{Kilgarriff2004,
abstract = {Word sketches are one-page automatic, corpus-based summaries of a words grammatical and collocational behaviour. They were first used in the production of the Macmillan English Dictionary and were presented at Euralex 2002. At that point, they only existed for English. Now, we have developed the Sketch Engine, a corpus tool which takes as input a corpus of any language and a corresponding grammar patterns and which generates word sketches for the words of that language. It also generates a thesaurus and sketch differences, which specify similarities and differences between near-synonyms. We briefly present a case study investigating applicability of the Sketch Engine to free word- order languages. The results show that word sketches could facilitate lexicographic work in Czech as they have for English.},
author = {Kilgarriff, Adam and Rychl\'{y}, Pavel and Smrz, Pavel and Tugwell, David},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/TheSketchEngineKilgarriff2004.pdf:pdf},
journal = {Linguistics},
pages = {105--116},
title = {{The Sketch Engine}},
url = {http://promethee.philo.ulg.ac.be/engdep1/download/bacIII/sketch-engine.pdf},
volume = {1},
year = {2004}
}
@inproceedings{Baskaya2013,
address = {Atlanta, Georgia, USA},
author = {Baskaya, Osman and Sert, Enis and Cirik, Volkan and Yuret, Deniz},
booktitle = {Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/AI-KU-Baskaya2013.pdf:pdf},
pages = {300--306},
title = {{AI-KU: Using Substitute Vectors and Co-Occurrence Modeling for Word Sense Induction and Disambiguation}},
url = {http://www.newdesign.aclweb.org/anthology-new/S/S13/S13-2050.pdf},
volume = {2},
year = {2013}
}
@article{Marneffe2010,
abstract = {PURPOSE: Normal neonates and many adults after abnormal visual development have directional preferences for visual stimulus motions; i.e., they give better responses for optokinetic nystagmus (OKN) and visually evoked potentials (VEPs) in one direction than to those in the opposite direction. The authors tested whether the VEP responses were asymmetrical because of abnormal eye movements. METHODS: VEPs were recorded from the visual cortices of five macaque monkeys: one normal, one neonate, and three reared with alternating monocular occlusion (AMO). They were lightly anesthetized, followed by paralysis to prevent eye movements. They then had "jittered" vertical grating patterns presented in their visual fields. The steady state VEPs were analyzed with discrete Fourier transforms to obtain the amplitudes and phases of the asymmetries. RESULTS: The normal, control monkey had small, insignificant amplitudes of its asymmetrical Fourier component and random phases that were not 180 degrees out of phase across the left and right eyes. The neonatal monkey and the AMO monkeys all had large, significant asymmetries that were approximately 180 degrees out of phase between the left and right eyes. CONCLUSIONS: The neonate and abnormally reared monkeys continued to have asymmetrical responses even after their eyes were paralyzed. Therefore, eye movements cannot be the source of the asymmetrical amplitudes of the VEPs, and the visual cortex is at least one source responsible for asymmetries observed in neonates and adults reared under abnormal visual inputs.},
author = {Marneffe, Marie-catherine De and Manning, Christopher D},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Marneffe, Manning - 2010 - Stanford typed dependencies manual.pdf:pdf},
institution = {Stanford University},
journal = {20090110 Httpnlp Stanford},
pages = {1--22},
pmid = {10476815},
publisher = {Citeseer},
title = {{Stanford typed dependencies manual}},
url = {http://nlp.stanford.edu/downloads/dependencies\_manual.pdf},
volume = {40},
year = {2010}
}
@article{Vinh2010,
abstract = {Information theoretic measures form a fundamental class of measures for comparing clusterings, and have recently received increasing interest. Nevertheless, a number of questions concerning their properties and inter-relationships remain unresolved. In this paper, we perform an organized study of information theoretic measures for clustering comparison, including several existing popular measures in the literature, as well as some newly proposed ones. We discuss and prove their important properties, such as the metric property and the normalization property. We then highlight to the clustering community the importance of correcting information theoretic measures for chance, especially when the data size is small compared to the number of clusters present therein. Of the available information theoretic based measures, we advocate the normalized information distance (NID) as a general measure of choice, for it possesses concurrently several important properties, such as being both a metric and a normalized measure, admitting an exact analytical adjusted-for-chance form, and using the nominal [0,1] range better than other normalized variants.},
author = {Vinh, Nx and Epps, J and Bailey, J},
doi = {10.1182/blood-2008-03-145946},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Vinh2010.pdf:pdf},
isbn = {9781605585161},
issn = {15280020},
journal = {The Journal of Machine Learning Research},
keywords = {adjustment for chance,clustering comparison,information theory,normalized infor-},
pages = {2837--2854},
pmid = {18544681},
title = {{Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance}},
url = {http://dl.acm.org/citation.cfm?id=1953024},
volume = {11},
year = {2010}
}
@inproceedings{Bruce1994,
abstract = {Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun "interest". We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data.},
address = {Las Cruces, New Mexico},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9406005},
author = {Bruce, Rebecca and Wiebe, Janyce},
booktitle = {Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics},
doi = {10.3115/981732.981752},
eprint = {9406005},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/P94-1020.pdf:pdf},
pages = {8},
primaryClass = {cmp-lg},
publisher = {Association for Computational Linguistics},
title = {{Word-Sense Disambiguation Using Decomposable Models}},
url = {http://arxiv.org/abs/cmp-lg/9406005},
year = {1994}
}
@misc{Hollingshead1949,
abstract = {Subtitled "An introduction to human ecology," this work attempts systematically to treat "least effort" (and its derivatives) as the principle underlying a multiplicity of individual and collective behaviors, variously but regularly distributed. The general orientation is quantitative, and the principle is widely interpreted and applied. After a brief elaboration of principles and a brief summary of pertinent studies (mostly in psychology), Part One (Language and the structure of the personality) develops 8 chapters on its theme, ranging from regularities within language per se to material on individual psychology. Part Two (Human relations: a case of intraspecies balance) contains chapters on "The economy of geography," "Intranational and international cooperation and conflict," "The distribution of economic power and social status," and "Prestige values and cultural vogues"-all developed in terms of the central theme},
author = {Hollingshead, August B. and Zipf, George Kingsley},
booktitle = {American Sociological Review},
doi = {10.2307/2086695},
isbn = {0444871500},
issn = {00031224},
pages = {822},
pmid = {1419},
title = {{Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology.}},
volume = {14},
year = {1949}
}
@inproceedings{Miller1993,
abstract = {A semantic concordance is a textual corpus and a lexicon So com- bined that every substantive word in the text is linked to its appropriate \~{}nse in the lexicon. Thus it can be viewed either as a corpus in which words have been tagged syntactically and semanti- cally, or as a lexicon in which example sentences can be found for many definitions. A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambigua- tion). The Brown Corpus is the text and WordNet is the lexicon. Semantic tags (pointers to WordNet synsets) are inserted in the text manually using an interface, ConText, that was designed to facili- tate the task. Another interface supports searches of the tagged text. Some practical uses for semantic concordances are proposed.},
address = {Princeton, New Jersey},
author = {Miller, George A. and Leacock, Claudia and Tengi, Randee and Bunker, Ross T.},
booktitle = {Proceedings of the Workshop on Human Language Technology - HLT '93},
doi = {10.3115/1075671.1075742},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/H93-1061.pdf:pdf},
isbn = {1558603247},
pages = {303--308},
title = {{A Semantic Concordance}},
year = {1993}
}
@inproceedings{Bordag2008,
abstract = {Observations of word co-occurrences and similarity compu- tations are often used as a straightforward way to represent the global contexts of words and achieve a simulation of semantic word similarity for applications such as word or document clustering and collocation ex- traction. Despite the simplicity of the underlying model, it is necessary to select a proper significance, a similarity measure and a similarity com- putation algorithm. However, it is often unclear how the measures are related and additionally often dimensionality reduction is applied to en- able the efficient computation of the word similarity. This work presents a linear time complexity approximative algorithm for computing word similarity without any dimensionality reduction. It then introduces a large-scale evaluation based on two languages and two knowledge sources and discusses the underlying reasons for the relative performance of each measure.},
address = {Haifa, Israel},
author = {Bordag, Stefan},
booktitle = {Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/BordagMC08.pdf:pdf},
pages = {52--63},
publisher = {Springer-Verlag},
title = {{A Comparison of Co-occurrence and Similarity Measures As Simulations of Context}},
volume = {4919 LNCS},
year = {2008}
}
@inproceedings{Lin1998,
abstract = {Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.},
address = {Montreal, Quebec, Canada},
author = {Lin, Dekang},
booktitle = {ACL '98 Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics},
doi = {10.3115/980432.980696},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Lin - 1998 - Automatic retrieval and clustering of similar words.pdf:pdf},
isbn = {0736-587X},
pages = {768--774},
publisher = {Association for Computational Linguistics},
title = {{Automatic retrieval and clustering of similar words}},
url = {http://portal.acm.org/citation.cfm?id=980696},
year = {1998}
}
@book{Riveiro2007,
abstract = {Information fusion is a field of research that strives to establish theories, techniques and tools that exploit synergies in data retrieved from multiple sources. In many real-world applications huge amounts of data need to be gathered, evaluated and analyzed in order to make the right decisions. An important key element of information fusion is the adequate presentation of the data that guides decision-making processes efficiently. This is where theories and tools developed in information visualization, visual data mining and human computer interaction (HCI) research can be of great support. This report presents an overview of information fusion and information visualization, highlighting the importance of the latter in information fusion research. Information vi- sualization techniques that can be used in information fusion are presented and analyzed providing insights into its strengths and weakness. Problems and challenges regarding the presentation of information that the decision maker faces in the ground situation awareness scenario (GSA) lead to open questions that are assumed to be the focus of further research.},
author = {Riveiro, Maria and Ziemke, Tom},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Riveiro, Ziemke - 2007 - Information Visualization for Information Fusion.pdf:pdf},
isbn = {9781595936707},
keywords = {decision support,human computer interaction,information fusion,information visualization,map-based visualization,situation aware- ness,uncertainty,visual data exploration,visual data mining},
mendeley-tags = {map-based visualization},
pages = {45},
title = {{Information Visualization for Information Fusion}},
year = {2007}
}
@article{Tishby2000,
abstract = {We define the relevant information in a signal \$x\backslash in X\$ as being the information that this signal provides about another signal \$y\backslash in \backslash Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \$\backslash X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \$\backslash X\$ that preserves the maximum information about \$\backslash Y\$. That is, we squeeze the information that \$\backslash X\$ provides about \$\backslash Y\$ through a `bottleneck' formed by a limited set of codewords \$\backslash tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,\backslash x)\$ emerges from the joint statistics of \$\backslash X\$ and \$\backslash Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X \backslash to \backslash tX\$ and \$\backslash tX \backslash to \backslash Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
archivePrefix = {arXiv},
arxivId = {physics/0004057},
author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
doi = {10.1108/eb040537},
eprint = {0004057},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/allerton.pdf:pdf},
pages = {1--11},
primaryClass = {physics},
title = {{The information bottleneck method}},
url = {http://arxiv.org/abs/physics/0004057},
year = {2000}
}
@phdthesis{Evert2005,
abstract = {You shall know a word by the company it keeps! With this slogan, Firth (1957) drew attention to a fact that language scholars had intuitively known for a long time: In natural language, words are not combined randomly into phrases and sentences, con- strained only by the rules of syntax. The particular ways in which they go together are a rich and important source of information both about language and about the world we live in. In the 1930s, J. R. Firth coined the term collocations for such char- acteristic, or habitual word combinations (as he called them). While Firth used to be lamentably vague about his precise understanding of this concept (cf. Lehr 1996, 21), the term itself and the general idea behind it that collocations correspond to some conventional way of saying things (Manning and Sch\"{u}tze 1999, 151) were eagerly taken up by researchers in various fields, leading to the serious terminolog- ical confusion that surrounds the concept of collocations today. As Choueka puts it: even though any two lexicographers would agree that once upon a time, hit the road and similar idioms are collocations, they would most certainly disagree on al- most anything else (Choueka 1988, 4). Feel free to replace lexicographers with any profession that is concerned with language data.},
author = {Evert, Stefan},
doi = {10.1073/pnas.141413598},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Evert2005phd.pdf:pdf},
issn = {00278424},
pages = {353},
pmid = {11447261},
school = {University of Stuttgart},
title = {{The Statistics of Word Cooccurrences Word Pairs and Collocations}},
url = {http://en.scientificcommons.org/19948039},
volume = {98},
year = {2005}
}
@article{Macropol2009,
author = {Macropol, Kathy},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/MCL\_Presentation2.pdf:pdf},
journal = {Ucsb},
number = {Mcl},
pages = {1--46},
title = {{Clustering on Graphs: The Markov Cluster Algorithm (MCL)}},
url = {papers3://publication/uuid/AA427443-A71F-4A64-BA2D-636B3E5A4911},
year = {2009}
}
@article{Cook2007,
abstract = {Sensors pervade our high-tech world - they link available computational power with physical applications. Sensors are rapidly catching up with computing devices in popularity and widespread use. As they become more varied and easy to use, the need to analyze sensor data grows. Scientists must understand sensor data for an extremely varied range of tasks, such as diagnosing automobiles, programming robots, analyzing traffic patterns, detecting terrorist threats, assuring the well-being of elderly persons living at home, and monitoring wildlife habitats.},
author = {Cook, Diane J},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Cook - 2007 - Making Sense of Sensor Data.pdf:pdf},
journal = {Sensors (Peterborough, NH)},
keywords = {anomaliy detection,pca,self-organizing maps,sensor data},
pages = {105--108},
title = {{Making Sense of Sensor Data}},
year = {2007}
}
@inproceedings{Zaharia2010,
abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However; Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs; and can be used to interactively query a 39 GB dataset with sub-second response time.; as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals; most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms},
address = {Boston, MA, USA},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
booktitle = {Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing (HotCloud'10)},
doi = {10.1007/s00256-009-0861-0},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/SparkZaharia2010.pdf:pdf},
issn = {03642348},
pages = {10},
title = {{Spark : Cluster Computing with Working Sets}},
year = {2010}
}
@book{VanRijsbergen1979,
abstract = {For a physician working as an expert continuous following of scientific literature is required. We elucidated the competence of 5th and 6th year students for the development of expertise. The mean time spent on reading medical literature was seven hours a week. The most important source of information for the students were websites with short quidelines and introductions written in students' own language. International original articles or English textbooks were not so much appreciated and seldom read. The present curricula in our medical schools do not encourage the student to search and acquire knowledge wider than their patients themselves do.},
author = {{Van Rijsbergen}, C J},
booktitle = {Information Retrieval},
doi = {10.1016/0020-0271(68)90016-8},
isbn = {0408709294},
issn = {00028231},
pages = {208},
pmid = {20923589},
publisher = {Butterworth},
title = {{Information Retrieval}},
volume = {30},
year = {1979}
}
@article{Rada1989,
abstract = {Motivated by the properties of spreading activation and conceptual distance, the authors propose a metric, called Distance, on the power set of nodes in a semantic net. Distance is the average minimum path length over all painvise combinations of nodes between two subsets of nodes. Distance can be successfully used to assess the conceptual distance between sets of concepts when used on a semantic net of hierarchical relations. When other kinds of relationships, like ‚Äúcause,‚Äù are used, Distance must be amended but then can again be effective. The judgments of Distance significantly correlate with the distance judgments that people make and help us determine whether semantic net SI is better or worse than semantic net S,. First a ‚Äúconceptual distance‚Äù task is set, and people are asked to perform it. Then the same task is performed by Distance on SI and \&. If Distance on SI performs more like people than Distance on S,, the conclusion is that SI is better than S,. Distance embedded in the methodology facilitates repeatable quantitative experiments.},
author = {Rada, R. and Mili, H. and Bicknell, E. and Blettner, M.},
doi = {10.1109/21.24528},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Rada et al. - 1989 - Development and application of a metric on semantic nets.pdf:pdf},
isbn = {0018-9472},
issn = {00189472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {1},
pages = {17--30},
title = {{Development and application of a metric on semantic nets}},
volume = {19},
year = {1989}
}
@inproceedings{Auer2007,
abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information can be made available on the Web for humans and machines. We describe some emerging applications from the DBpedia community and show how website operators can reduce costs by facilitating royalty-free DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data sources.},
address = {Busan, Korea},
author = {Auer, S\"{o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
booktitle = {Proceedings of the 6th International The Semantic Web and 2Nd Asian Conference on Asian Semantic Web Conference},
doi = {10.1007/978-3-540-76298-0\_52},
isbn = {3540762973},
issn = {03029743},
pages = {722--735},
title = {{DBpedia: A nucleus for a Web of open data}},
volume = {4825 LNCS},
year = {2007}
}
@article{Kraak2007,
author = {Kraak, Menno-Jan},
doi = {10.3138/carto.42.2.115},
issn = {03177173},
journal = {Cartographica},
number = {2},
pages = {115--116},
title = {{Geovisualization and visual analytics}},
url = {http://utpjournals.metapress.com/openurl.asp?genre=article\&id=doi:10.3138/carto.42.2.115},
volume = {42},
year = {2007}
}
@article{Biemann2013,
author = {Biemann, Chris and Riedl, Martin},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/BiemannRiedlText2D\_JLM2013\_60-369-1-PB.pdf:pdf},
issn = {2299-8470},
journal = {Journal of Language Modelling},
number = {1},
pages = {55--95},
title = {Text: {N}ow in {2D}! a framework for lexical expansion with contextual similarity},
url = {http://bach.ipipan.waw.pl/ojs/index.php/JLM/article/view/60},
volume = {1},
year = {2013}
}
@inproceedings{Widdows2002,
abstract = {This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82\% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word.},
address = {Taipei, Taiwan},
author = {Widdows, Dominic and Dorow, Beate},
booktitle = {Proceedings of the 19th international conference on Computational linguistics},
doi = {10.3115/1072228.1072342},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Widdows2002.pdf:pdf},
pages = {1--7},
title = {{A graph model for unsupervised lexical acquisition}},
url = {http://portal.acm.org/citation.cfm?doid=1072228.1072342},
year = {2002}
}
@book{Cimiano2006,
abstract = {In the last decade, ontologies have received much attention within computer science and related disciplines, most often as the semantic web. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications discusses ontologies for the semantic web, as well as knowledge management, information retrieval, text clustering and classification, as well as natural language processing.},
author = {Cimiano, Philipp},
booktitle = {Ontology Learning and Population from Text: Algorithms, Evaluation and Applications},
pages = {1--347},
publisher = {Springer-Verlag},
title = {{Ontology learning and population from text: Algorithms, evaluation and applications}},
year = {2006}
}
@article{Rapp2004,
author = {Rapp, R},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Rapp2004.pdf:pdf},
journal = {SDM},
title = {{Mining Text for Word Senses Using Independent Component Analysis.}},
url = {http://books.google.de/books?hl=de\&lr=\&id=gcJVK9a9RR0C\&oi=fnd\&pg=PA422\&dq=Mining+text+for+word+senses+using+independent+component+analysis\&ots=mPph2TAf6i\&sig=qDk622yWmHqy72e0Bff3LS0BeXk},
year = {2004}
}
@article{Amigo2009,
address = {Hingham, MA, USA},
author = {Amig\'{o}, Enrique and Gonzalo, Julio and Artiles, Javier and Verdejo, Felisa},
doi = {10.1007/s10791-008-9066-8},
issn = {1386-4564},
journal = {Information Retrieval},
keywords = {Clustering,Evaluation metrics,Formal constraints},
month = aug,
number = {4},
pages = {461--486},
publisher = {Kluwer Academic Publishers},
title = {{A Comparison of Extrinsic Clustering Evaluation Metrics Based on Formal Constraints}},
url = {http://dx.doi.org/10.1007/s10791-008-9066-8},
volume = {12},
year = {2009}
}
@article{Martinez-villalpando2009,
author = {Martinez-villalpando, Ernesto C and Herr, Hugh},
doi = {10.1682/JRRD.2008.09.0131},
keywords = {agonist-antagonist actuation,biomimetic design,fin},
number = {3},
pages = {361--373},
title = {{Agonist-antagonist active knee prosthesis: A preliminary study in level-ground walking}},
volume = {46},
year = {2009}
}
@inproceedings{Curran2002,
abstract = {Ensemble methods are state of the art for many NLP tasks. Recent work by Banko and Brill (2001) suggests that this would not necessarily be true if very large training corpora were available. However, their results are limited by the simplic- ity of their evaluation task and individual classifiers. Our work explores ensemble efficacy for the more complex task of automatic the- saurus extraction on up to 300 million words. We examine our conflicting results in terms of the constraints on, and com- plexity of, different contextual representa- tions, which contribute to the sparseness- and noise-induced bias behaviour of NLP systems on very large corpora},
address = {Philadelphia, PA},
author = {Curran, James R.},
booktitle = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing - EMNLP '02},
doi = {10.3115/1118693.1118722},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Curran2002.pdf:pdf},
pages = {222--229},
publisher = {Association for Computational Linguistics},
title = {{Ensemble methods for automatic thesaurus extraction}},
url = {http://portal.acm.org/citation.cfm?doid=1118693.1118722},
volume = {10},
year = {2002}
}
@inproceedings{Agirre2009,
abstract = {In this paper we propose a new graph- based method that uses the knowledge in a LKB (based on WordNet) in order to performunsupervisedWord Sense Disam- biguation. Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithmcan be easily ported to other lan- guages with good results, with the only re- quirement of having a wordnet. In addi- tion, we make an analysis of the perfor- mance of the algorithm, showing that it is efficient and that it could be tuned to be faster.},
author = {Agirre, Eneko and Soroa, Aitor},
booktitle = {Proceedings of the 12th Conference of the European Chapter of the ACL},
doi = {10.3115/1609067.1609070},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/PersonalizedPageRankAgirre2009.pdf:pdf},
isbn = {9781932432169},
pages = {33--41},
title = {{Personalizing PageRank for Word Sense Disambiguation}},
year = {2009},
address = {Athens, Greece},

}

@article{Agirre2007-ALT,
abstract = {The goal of this task is to allow for com- parison across sense-induction and discrim- ination systems, and also to compare these systems to other supervised and knowledge- based systems. In total there were 6 partic- ipating systems. We reused the SemEval- 2007 English lexical sample subtask of task 17, and set up both clustering-style unsuper- vised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sam- ple subtask of task 17.},
author = {Agirre, Eneko and Soroa, Aitor},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Agirre2007-SemEval2007Task2.pdf:pdf},
journal = {Computational Linguistics},
pages = {7--12},
title = {{Semeval-2007 Task 02 : Evaluating Word Sense Induction and Discrimination Systems}},
url = {http://www.aclweb.org/anthology/W/W07/W07-2002},
year = {2007}
}


@article{Bizer2014,
author = {Bizer, Christian},
file = {:Volumes/Bill/Documents/Uni/Watson-Projekt/Papers/dbpedia.pdf:pdf},
journal = {Semantic Web},
keywords = {corresponding author,e-mail,informatik,knowledge extraction,lehmannn,linked data,multilingual knowledge bases,rdf,uni-,wikipedia},
pages = {1--5},
title = {{DBpedia - A Large-scale , Multilingual Knowledge Base Extracted from Wikipedia}},
volume = {1},
year = {2014}
}
@inproceedings{Agirre2006,
address = {New York City, USA},
author = {Agirre, Eneko and Mart\'{\i}nez, David and Lacalle, Oier L De and Soroa, Aitor},
booktitle = {Proceedings of TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/Agirre2006.pdf:pdf},
pages = {89--96},
publisher = {Association for Computational Linguistics},
title = {{Evaluating and optimizing the parameters of an unsupervised graph-based WSD algorithm}},
url = {http://www.aclweb.org/anthology/W/W06/W06-2214},
year = {2006}
}
@article{KLOECKL,
abstract = {The increasing pervasiveness of urban systems and networks utilizing digital technologies for their operation generates enormous amounts of digital traces capable of reflecting in real-time how people make use of space and infrastructures in the city. This is not only transforming how we study, design and manage cities but opens up new possibilities for tools that give people access to up-to-date information about urban dynamics, allowing them to take decisions that are more in sync with their environment. LIVE Singapore! explores the development of an open platform for the collection, elaboration and distribution of a large and growing number of different kinds of real-time data that originate in a city. Inspired by recent data.gov initiatives, the platform is structured to become itself a tool for developer communities, allowing them to analyze data and write applications that create links between a city's different real-time data streams, offering new insights and services to citizens. Being a compact island based city-state metropolis, Singapore offers a unique context for this study. This paper addresses the value of stream data for city planning and management as well as modalities to give citizens meaningful access to large amounts of data capable of informing their decisions. We describe the technology context within which this project is framed, illustrate the requirements and the architecture of the open real-time data platform to serve as a base for programming the city, and finally we present and discuss the first platform prototype (using real-world data from operators},
author = {KLOECKL, K. and SENN, O.},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/KLOECKL, SENN - Unknown - LIVE Singapore!-An urban platform for real-time data to program the city.pdf:pdf},
journal = {senseable.mit.edu},
keywords = {control system,map-based visualization,open data platform,real-time city,singapore,urban dynamics,urban management systems},
mendeley-tags = {map-based visualization,singapore},
title = {{LIVE Singapore!-An urban platform for real-time data to program the city}},
url = {http://senseable.mit.edu/papers/pdf/2011\_Kloeckl\_et\_al\_LIVESingapore\_CUPUM.pdf}
}


@article{Cucerzan2007,
abstract = {This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.},
author = {Cucerzan, Silviu},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/emnlp07.pdf:pdf},
isbn = {9788890354175},
issn = {02120062},
journal = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
pages = {708--716},
title = {{Large-Scale Named Entity Disambiguation Based on Wikipedia Data}},
url = {http://research.microsoft.com/pubs/68124/emnlp07.pdf},
volume = {2007},
year = {2007}
}


@inproceedings{Manandhar2010,
abstract = {This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction \& Disambiguation task, as well as the evaluation results of 26 participating systems. In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses. Systems‚Äô answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.},
address = {Uppsala, Sweden},
author = {Manandhar, Suresh},
booktitle = {Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval 2010)},
file = {:Users/jsimon/Dropbox/MA-Ausarbeitung/Papers/SemEval-2010-Task14.pdf:pdf},
pages = {63--68},
title = {{SemEval-2010 Task 14 : Word Sense Induction \& Disambiguation}},
year = {2010}
}





@article{Matsuo2001,
author = {Matsuo, Yutaka and Ohsawa, Yukio and Ishizuka, Mitsuro},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Matsuo, Ohsawa, Ishizuka - 2001 - Keyworld Extracting keywords from document s small world.pdf:pdf},
journal = {Discovery Science},
title = {{Keyworld: Extracting keywords from document s small world}},
url = {http://link.springer.com/chapter/10.1007/3-540-45650-3\_24},
year = {2001}
}
@article{Maciejewskia,
abstract = {As data sources become larger and more complex, the ability to effectively explore and analyze patterns among varying sources becomes a critical bottleneck in analytic reasoning. Incoming data contain multiple variables, high signal-to-noise ratio, and a degree of uncertainty, all of which hinder exploration, hypothesis generation/exploration, and decision making. To facilitate the exploration of such data, advanced tool sets are needed that allow the user to interact with their data in a visual environment that provides direct analytic capability for finding data aberrations or hotspots. In this paper, we present a suite of tools designed to facilitate the exploration of spatiotemporal data sets. Our system allows users to search for hotspots in both space and time, combining linked views and interactive filtering to provide users with contextual information about their data and allow the user to develop and explore their hypotheses. Statistical data models and alert detection algorithms are provided to help draw user attention to critical areas. Demographic filtering can then be further applied as hypotheses generated become fine tuned. This paper demonstrates the use of such tools on multiple geospatiotemporal data sets.},
author = {Maciejewski, Ross and Rudolph, Stephen and Hafen, Ryan and Abusalah, Ahmad M and Yakout, Mohamed and Ouzzani, Mourad and Cleveland, William S and Grannis, Shaun J and Ebert, David S},
doi = {10.1109/TVCG.2009.100},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Keim et al. - Unknown - Advanced Visual Analytics Interfaces.pdf:pdf},
issn = {1077-2626},
journal = {IEEE transactions on visualization and computer graphics},
keywords = {Algorithms,Artificial Intelligence,Computer Graphics,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Image Interpretation,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models,Theoretical,User-Computer Interface,hotspots,map-based visualization},
mendeley-tags = {hotspots,map-based visualization},
number = {2},
pages = {205--20},
pmid = {20075482},
title = {{A visual analytics approach to understanding spatiotemporal hotspots.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20075482},
volume = {16}
}
@misc{Crimereports,
title = {{Crimereports Project, https://www.crimereports.com/}},
url = {https://www.crimereports.com/}
}
@inproceedings{Biemann2010,
abstract = {This paper examines the influence of features based on clusters of co-occurrences for supervised Word Sense Disambigua- tion and Lexical Substitution. Co- occurrence cluster features are derived from clustering the local neighborhood of a target word in a co-occurrence graph based on a corpus in a completely un- supervised fashion. Clusters can be as- signed in context and are used as features in a supervised WSD system. Experi- ments fitting a strong baseline system with these additional features are conducted on two datasets, showing improvements. Co- occurrence features are a simple way to mimic Topic Signatures (Mart¬¥ 2008) without needing to construct re- sources manually. Further, a system is de- scribed that produces lexical substitutions in context with very high precision},
address = {Uppsala, Sweden},
author = {Biemann, Chris},
booktitle = {Proceedings of the 5th Workshop on TextGraphs in conjunction with ACL},
file = {:Users/jsimon/Library/Application Support/Mendeley Desktop/Downloaded/Biemann - 2010 - Co-occurrence cluster features for lexical substitutions in context.pdf:pdf},
isbn = {1932432779},
pages = {55--59},
publisher = {Association for Computational Linguistics},
title = {{Co-Occurrence Cluster Features for Lexical Substitutions in Context}},
url = {http://dl.acm.org/citation.cfm?id=1870499},
year = {2010}
}

@inproceedings{Baroni2004,
  author      = {Baroni, Marco and Bernardini, Silvia},
  title       = { {BootCaT}: Bootstrapping Corpora and Terms from the Web},
  booktitle   = {Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC)},
  pages = {1313--1316},
  year        = {2004},
  address     = {Lisbon, Portugal}}https://www.sharelatex.com/project/56b0aabf4e354a1327a73881

@inproceedings{Remus2016,
  author      = {Remus, Steffen and Biemann, Chris},
  title       = {Domain-Specific Corpus Expansion with Focused Webcrawling},
  booktitle   = {Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC)},
  year        = {2016},
  address     = {Portoro\v{z}, Slovenia}}

@inproceedings{Lefever2014,
  author      = {Lefever, Els and Van de Kauter, Marjan and Hoste, V{\'e}ronique},
  title       = {Hypoterm: Detection of hypernym relations between domain-specific terms in dutch and english},
  journal     = {Terminology},
  volume      = {20},
  number      = {2},
  pages       = {250--278},
  year        = {2014},
  publisher   = {John Benjamins Publishing Company}}

// from Stefano  
@inproceedings{webisa2016,
address = {Portoro\v{z}, Slovenia},
author = {Seitner,Julian and Bizer,Christian and Eckert,Kai and Faralli,Stefano and Meusel,Robert and Paulheim,Heiko and Simone Ponzetto},
booktitle = {Proceedings of the 10th edition of the Language Resources and Evaluation Conference},
title = {{A Large DataBase of Hypernymy Relations Extracted from the Web}},
year = {2016}
}

@inproceedings{Farallietal:15,
  author    = {Stefano Faralli and
               Giovanni Stilo and
               Paola Velardi},
  title     = {Large Scale Homophily Analysis in Twitter Using a Twixonomy},
  booktitle = {Proceedings of the Twenty-Fourth International Joint Conference on
               Artificial Intelligence, {IJCAI} 2015},
  pages     = {2334--2340},
  year      = {2015},
  crossref  = {DBLP:conf/ijcai/2015},
  url       = {http://ijcai.org/papers15/Abstracts/IJCAI15-330.html},
  timestamp = {Mon, 20 Jul 2015 19:12:40 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ijcai/FaralliSV15},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  address={Buenos Aires, Argentina}
}

@article{Tarjan:72,
  author = {Tarjan, Robert},
  journal = {SIAM Journal on Computing},
  pages = {146--160},
  title = {Depth-first search and linear graph algorithms},
  volume = 1,
  year = 1972
}

@inproceedings{Oakes:05,
author = {Michael P. Oakes},
title = {Using {H}earst's Rules for the Automatic Acquisition of Hyponyms for Mining a Pharmaceutical Corpus},
booktitle = "RANLP Text Mining Workshop'05",
address={Borovets, Bulgaria},
pages = {63-67},
year = {2005}, 
}
@inproceedings{Agirreetal:00,
  author    = {Eneko Agirre and
               Olatz Ansa and
               Eduard H. Hovy and
               David Mart\'{\i}nez},
  title     = {Enriching very large ontologies using the {WWW}},
  booktitle = {ECAI Workshop on Ontology Learning},
  year      = {2000},
  pages = {28--33},
  address =  {Berlin, Germany}
}
@inproceedings{Caraballo:99,
  author = 	 {Sharon A. Caraballo},
  title = 	 {Automatic construction of a hypernym-labeled noun hierarchy from text},
  booktitle = {Proceedings of the $37^{th}$ Annual Meeting of the Association for Computational Linguistics: Proceedings of the Conference},
  address = {Maryland, USA},
  pages = {120--126},
  year =	{1999},
}
@inproceedings{Snowetal:04,
  author =       {Snow, Rion and Jurafsky, Daniel and Ng, Andrew  Y.},
  title =        {Learning Syntactic Patterns for Automatic Hypernym Discovery},
  booktitle =    nips-2004s,
  pages =        {1297-1304},
  publisher =    {MIT Press},
  year =         2004,
  editor =       {Saul, Lawrence  K. and Weiss, Yair and Bottou, L\'{e}on},
  address =      {Cambridge, Mass.},
  keywords =     {parsing, statistical parsing}
}
@inproceedings{Snowetal:06,
   author = {Rion Snow and Dan Jurafsky and Andrew Ng},
   title =        {Semantic taxonomy induction from heterogeneous evidence},
   booktitle =    coling-06s,
   pages =        {801-808},
   year =         2006,
   address={Sydney, Australia}
 }
 @inproceedings{Ritteretal:09,
   author = {Alan Ritter and Stephen Soderland and Oren Etzioni},
   title = {What is this, anyway: Automatic hypernym discovery},
   booktitle = {Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read},
   address={Palo Alto, California},
   year = "2009",
   pages ={88--93}
}
@inproceedings{Cimianoetal:05b,
  author = 	 {Philipp, Cimiano and Gunter, Ladwig and Steffen, Staab},
  title = 	 {Gimme the context: context-driven automatic semantic annotation with C-PANKOW},
  booktitle = www-05s,
  address = {Chiba, Japan},
  year =	{2005},
}
@inproceedings{YangCallan:09,
 author = {Yang, Hui and Callan, Jamie},
 title = {A metric-based framework for automatic taxonomy induction},
 booktitle = acl-09,
 year = {2009},
 isbn = {978-1-932432-45-9},
 address = {Suntec, Singapore},
 pages = {271--279},
 numpages = {9},
 url = {http://portal.acm.org/citation.cfm?id=1687878.1687918},
 acmid = {1687918}
} 
@inproceedings{Pasca:04,
 author = {Pasca, Marius},
 title = {Acquisition of categorized named entities for web search},
 booktitle = {Proceedings of the thirteenth ACM international conference on Information and knowledge management (CIKM)},
 year = {2004},
 isbn = {1-58113-874-1},
 address = {Washington, D.C., USA},
 pages = {137--145},
} 
@article{Cimianoetal:05a,
  author = {Cimiano, Philipp and Hotho, Andreas and Staab, Steffen},
  interhash = {4c09568cff62babd362aab03095f4589},
  intrahash = {8299d264161ecd740168c89b781f84ae},
  journal = {Journal of Artificial Intelligence Research},
  number = 1,
  pages = {305-339},
  title = {Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis},
  url = {http://ontology.csse.uwa.edu.au/reference/browse_paper.php?pid=233281549},
  volume = 24,
  year = 2005,
  keywords = {imported},
  added-at = {2010-11-24T09:28:52.000+0100},
  biburl = {http://www.bibsonomy.org/bibtex/28299d264161ecd740168c89b781f84ae/wyswilson}
}
@inproceedings{Tangetal:09,
  author = {Tang, Jie and fung Leung, Ho and Luo, Qiong and Chen, Dewei and Gong, Jibin},
  address = {San Francisco, CA, USA},
  booktitle = ijcai-09s,
  pages = {2089--2094},
  title = {Towards Ontology Learning from Folksonomies},
  year = 2009,
  location = {Pasadena, California, USA},
}
@inproceedings{PoonDomingos:10,
 author = {Poon, Hoifung and Domingos, Pedro},
 title = {Unsupervised ontology induction from text},
 booktitle = {Proceedings of ACL 2010},
 year = {2010},
 location = {Uppsala, Sweden},
 pages = {296--305},
 numpages = {10},
 url = {http://portal.acm.org/citation.cfm?id=1858681.1858712},
 acmid = {1858712},
 address = {Stroudsburg, USA},
} 
@InProceedings{KozarevaHovy:10a,
  author    = {Kozareva, Zornitsa  and  Hovy, Eduard},
  title     = {Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns},
  booktitle = acl-10,
  year      = {2010},
  address   = {Uppsala, Sweden},
  pages     = {1482--1491},
  url       = {http://www.aclweb.org/anthology/P10-1150}
}
@article{chakrabarti:99,
  author      = {Soumen Chakrabarti and Martin van den Berg and Byron Dom},
  journal     = {Computer Networks},
  number      = {11--16},
  pages       = {1623--1640},
  title       = {Focused crawling: a new approach to topic-specific Web resource discovery},
  volume      = {31},
  year        = {1999}}
  
  
  
@InProceedings{navigli2010babelnet,
  title={{BabelNet:} Building a very large multilingual semantic network},
  author={Navigli, Roberto and Ponzetto, Simone Paolo},
  booktitle = acl-10,
  address   = {Uppsala, Sweden},
  pages={216--225},
  year={2010}
}
